{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting PPO agent experiment with LSTM policy for SBEOS environment\n",
      "\n",
      "=== Training Phase ===\n",
      "State dimension: 31, Action dimension: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training PPO with LSTM:   2%|▏         | 10/500 [00:28<22:40,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: -1286.90, Avg Reward (last 10): -598.52, Accuracy: 0.4020, Steps: 500\n",
      "New best model saved! Avg reward: -598.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   2%|▏         | 11/500 [00:30<22:23,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: -409.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   2%|▏         | 12/500 [00:33<21:52,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: -92.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   3%|▎         | 13/500 [00:36<22:28,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 121.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   4%|▍         | 19/500 [00:52<20:55,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, Reward: -863.20, Avg Reward (last 10): -247.66, Accuracy: 0.4420, Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   4%|▍         | 20/500 [00:55<23:33,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 20: Reward = -1332.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   6%|▌         | 28/500 [01:17<20:43,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 373.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   6%|▌         | 30/500 [01:22<20:38,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30, Reward: 2637.20, Avg Reward (last 10): 544.36, Accuracy: 0.6100, Steps: 500\n",
      "New best model saved! Avg reward: 544.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   6%|▌         | 31/500 [01:25<21:02,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 811.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   6%|▋         | 32/500 [01:27<20:55,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 879.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   7%|▋         | 33/500 [01:31<21:54,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1002.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   7%|▋         | 34/500 [01:33<21:55,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1014.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   8%|▊         | 39/500 [01:47<20:32,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, Reward: 395.60, Avg Reward (last 10): 64.23, Accuracy: 0.4860, Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   8%|▊         | 40/500 [01:51<23:52,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 40: Reward = 1454.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import copy\n",
    "import math\n",
    "from environments import SBEOS_Environment\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a transition tuple for PPO memory\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'reward', 'next_state', 'done', 'log_prob', 'value'))\n",
    "\n",
    "class PPOMemory:\n",
    "    \"\"\"Memory buffer for PPO algorithm\"\"\"\n",
    "    def __init__(self, capacity=2000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition to the memory buffer\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Return all transitions in the buffer\"\"\"\n",
    "        return Transition(*zip(*self.memory))\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the memory buffer\"\"\"\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    \"\"\"LSTM-based feature extractor for processing sequential data\"\"\"\n",
    "    def __init__(self, state_size, hidden_dim=64, num_layers=2):\n",
    "        super(LSTMFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Separate parts of the state\n",
    "        self.window_size = state_size - 6  # Raw window part of state\n",
    "        \n",
    "        # For processing the raw window\n",
    "        self.window_layer = nn.Sequential(\n",
    "            nn.Linear(self.window_size, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # For processing the statistical features\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(6, 16),  # 6 statistical features\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.pre_lstm_layer = nn.Sequential(\n",
    "            nn.Linear(32 + 16, 64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequence processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Handle different input shapes\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = 1 if x.dim() == 2 else x.size(1)\n",
    "        \n",
    "        if x.dim() == 2:  # [batch_size, features]\n",
    "            x = x.unsqueeze(1)  # Add sequence dimension\n",
    "            \n",
    "        # Reshape for processing\n",
    "        x_reshaped = x.view(batch_size * seq_len, -1)\n",
    "        \n",
    "        # Split input into raw window and statistical features\n",
    "        window = x_reshaped[:, :self.window_size]\n",
    "        features = x_reshaped[:, self.window_size:]\n",
    "        \n",
    "        # Process through respective paths\n",
    "        window_features = self.window_layer(window)\n",
    "        stat_features = self.feature_layer(features)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([window_features, stat_features], dim=1)\n",
    "        pre_lstm = self.pre_lstm_layer(combined)\n",
    "        \n",
    "        # Reshape back to [batch_size, seq_len, features]\n",
    "        pre_lstm = pre_lstm.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(pre_lstm)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(pre_lstm, hidden)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Return the final output for each sequence\n",
    "        return lstm_out, hidden\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Combined Actor-Critic network with LSTM backbone\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Feature extraction with LSTM\n",
    "        self.feature_extractor = LSTMFeatureExtractor(state_size, hidden_dim=hidden_dim)\n",
    "        \n",
    "        # Policy network (Actor)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, action_size),\n",
    "        )\n",
    "        \n",
    "        # Value network (Critic)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Extract features\n",
    "        features, hidden = self.feature_extractor(x, hidden)\n",
    "        \n",
    "        # Get last sequence outputs if sequence data\n",
    "        if features.dim() > 2:\n",
    "            features = features[:, -1]\n",
    "        \n",
    "        # Calculate policy and value\n",
    "        action_probs = F.softmax(self.actor(features), dim=-1)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_probs, value, hidden\n",
    "    \n",
    "    def get_action(self, state, hidden=None, evaluation=False):\n",
    "        \"\"\"Select an action based on the policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action_probs, value, hidden = self.forward(state, hidden)\n",
    "            \n",
    "            if evaluation:\n",
    "                # Deterministic action for evaluation\n",
    "                action = torch.argmax(action_probs, dim=-1)\n",
    "            else:\n",
    "                # Stochastic action for training\n",
    "                dist = Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "            \n",
    "        if evaluation:\n",
    "            return action[0].item(), hidden\n",
    "        else:\n",
    "            # return action.item(), log_prob, value.squeeze(), hidden\n",
    "            return action[0].item(), log_prob[0], value[0].squeeze(), hidden\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        hidden_dim=128,\n",
    "        learning_rate=0.0003,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        max_grad_norm=0.5,\n",
    "        ppo_epochs=4,\n",
    "        batch_size=64,\n",
    "        seq_len=8,\n",
    "        use_gae=True,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gae = use_gae\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Initialize the actor-critic network\n",
    "        self.policy = ActorCritic(state_size, action_size, hidden_dim=hidden_dim).to(device)\n",
    "        \n",
    "        # Initialize optimizer with learning rate scheduler\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=10, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Initialize memory buffer\n",
    "        self.memory = PPOMemory(capacity=2000)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.rewards_history = []\n",
    "        self.losses = []\n",
    "        self.entropy_history = []\n",
    "        self.predictions = []\n",
    "        self.true_states = []\n",
    "        \n",
    "        # Sequence buffer for LSTM states\n",
    "        self.state_buffer = []\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        \"\"\"Store a transition in memory\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done, log_prob, value)\n",
    "    \n",
    "    def get_action(self, state, hidden=None, evaluation=False):\n",
    "        \"\"\"Select an action using the policy\"\"\"\n",
    "        # Convert state to tensor\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor([state]).to(device)\n",
    "            \n",
    "        # Make sure it has batch dimension\n",
    "        if state_tensor.dim() == 1:\n",
    "            state_tensor = state_tensor.unsqueeze(0)\n",
    "        \n",
    "        return self.policy.get_action(state_tensor, hidden, evaluation)\n",
    "    \n",
    "    def compute_returns(self, rewards, dones, values, next_value):\n",
    "        \"\"\"Compute returns using Generalized Advantage Estimation (GAE)\"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        \n",
    "        # Initialize advantage and return\n",
    "        advantage = 0\n",
    "        next_value = next_value.detach()\n",
    "        \n",
    "        # Reverse iterate through rewards and values\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                # For the last timestep\n",
    "                next_nonterminal = 1.0 - dones[t]\n",
    "                next_return = next_value\n",
    "            else:\n",
    "                next_nonterminal = 1.0 - dones[t+1]\n",
    "                next_return = returns[0]\n",
    "            \n",
    "            # Compute TD error\n",
    "            delta = rewards[t] + self.gamma * next_nonterminal * next_value - values[t]\n",
    "            \n",
    "            # Compute advantage using GAE\n",
    "            advantage = delta + self.gamma * self.gae_lambda * next_nonterminal * advantage\n",
    "            \n",
    "            # Update return and next value\n",
    "            returns.insert(0, advantage + values[t])\n",
    "            next_value = values[t]\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy using PPO algorithm\"\"\"\n",
    "        # Check if we have enough transitions\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get all transitions from memory\n",
    "        transitions = self.memory.sample()\n",
    "        \n",
    "        # Convert transitions to tensors\n",
    "        states = torch.FloatTensor(np.array(transitions.state)).to(device)\n",
    "        actions = torch.LongTensor(transitions.action).to(device)\n",
    "        rewards = torch.FloatTensor(transitions.reward).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(transitions.next_state)).to(device)\n",
    "        dones = torch.FloatTensor(transitions.done).to(device)\n",
    "        old_log_probs = torch.FloatTensor(transitions.log_prob).to(device)\n",
    "        old_values = torch.FloatTensor(transitions.value).to(device)\n",
    "        \n",
    "        # Get the last state value to bootstrap returns\n",
    "        with torch.no_grad():\n",
    "            _, next_value, _ = self.policy(states[-1:])\n",
    "            next_value = next_value.squeeze()\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = self.compute_returns(rewards, dones, old_values, next_value)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # PPO update\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Split into batches\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # Generate random indices for minibatches\n",
    "            indices = np.random.permutation(len(states))\n",
    "            \n",
    "            # Process in batches\n",
    "            for start_idx in range(0, len(states), self.batch_size):\n",
    "                # Get minibatch indices\n",
    "                batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "                \n",
    "                # Get minibatch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_old_values = old_values[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                \n",
    "                # Reset hidden state for each batch\n",
    "                hidden = None\n",
    "                \n",
    "                # Get current policy outputs\n",
    "                action_probs, current_values, _ = self.policy(batch_states, hidden)\n",
    "                current_values = current_values.squeeze()\n",
    "                \n",
    "                # Calculate advantage\n",
    "                advantages = batch_returns - batch_old_values\n",
    "                # Normalize advantages\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                # Calculate log probs\n",
    "                dist = Categorical(action_probs)\n",
    "                current_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Calculate ratios and clipped objective\n",
    "                ratios = torch.exp(current_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss (using clipped value loss)\n",
    "                value_loss = F.mse_loss(current_values, batch_returns)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = actor_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Optimize the model\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # Clear memory after update\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Return average loss\n",
    "        return total_loss / (self.ppo_epochs * (len(states) // self.batch_size + 1))\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filepath)\n",
    "        \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def train_ppo_agent(env, agent, episodes=500, max_steps=500, update_interval=1024, eval_freq=10):\n",
    "    \"\"\"Train the PPO agent\"\"\"\n",
    "    # Record best model for early stopping\n",
    "    best_reward = float('-inf')\n",
    "    best_model = None\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    all_predictions = []\n",
    "    all_true_states = []\n",
    "    training_losses = []\n",
    "    entropy_history = []\n",
    "    \n",
    "    # Episode window for tracking improvement\n",
    "    window_size = 10\n",
    "    window_rewards = deque(maxlen=window_size)\n",
    "    \n",
    "    # Episodic LSTM state buffer\n",
    "    state_sequence = deque(maxlen=agent.seq_len)\n",
    "    \n",
    "    # Step counter for PPO updates\n",
    "    step_count = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(episodes), desc=\"Training PPO with LSTM\"):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        # Initialize LSTM hidden state\n",
    "        hidden = None\n",
    "        state_sequence.clear()\n",
    "        \n",
    "        # Collect episode data\n",
    "        episode_predictions = []\n",
    "        episode_true_states = []\n",
    "        \n",
    "        # Run episode\n",
    "        while not done and steps < max_steps:\n",
    "            # Add current state to sequence buffer\n",
    "            state_sequence.append(state)\n",
    "            \n",
    "            # Pad sequence if needed\n",
    "            while len(state_sequence) < agent.seq_len:\n",
    "                state_sequence.appendleft(state)\n",
    "            \n",
    "            # Convert sequence to numpy array\n",
    "            state_seq = np.array(list(state_sequence))\n",
    "            \n",
    "            # Select action\n",
    "            action, log_prob, value, hidden = agent.get_action(state_seq, hidden)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Record prediction and true state\n",
    "            episode_predictions.append(action)\n",
    "            episode_true_states.append(info[\"state\"])\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done, log_prob, value)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            step_count += 1\n",
    "            \n",
    "            # Update policy if enough steps have been accumulated\n",
    "            if step_count >= update_interval:\n",
    "                loss = agent.update_policy()\n",
    "                episode_loss += loss\n",
    "                step_count = 0\n",
    "        \n",
    "        # Force update at end of episode if we have data\n",
    "        if len(agent.memory) > 0:\n",
    "            loss = agent.update_policy()\n",
    "            episode_loss += loss\n",
    "            step_count = 0\n",
    "        \n",
    "        # Record episode metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        all_predictions.extend(episode_predictions)\n",
    "        all_true_states.extend(episode_true_states)\n",
    "        window_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calculate accuracy for the episode\n",
    "        episode_accuracy = np.mean(np.array(episode_predictions) == np.array(episode_true_states))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            window_avg = np.mean(window_rewards)\n",
    "            print(f\"Episode: {episode+1}, Reward: {episode_reward:.2f}, Avg Reward (last {window_size}): {window_avg:.2f}, \"\n",
    "                  f\"Accuracy: {episode_accuracy:.4f}, Steps: {steps}\")\n",
    "            \n",
    "            # Update learning rate based on performance\n",
    "            agent.scheduler.step(window_avg)\n",
    "        \n",
    "        # Save best model\n",
    "        if len(window_rewards) == window_size:\n",
    "            window_avg = np.mean(window_rewards)\n",
    "            if window_avg > best_reward:\n",
    "                best_reward = window_avg\n",
    "                best_model = copy.deepcopy(agent.policy.state_dict())\n",
    "                no_improvement_count = 0\n",
    "                print(f\"New best model saved! Avg reward: {best_reward:.2f}\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "        \n",
    "        # Early stopping if no improvement for a while\n",
    "        if no_improvement_count >= 30:\n",
    "            print(f\"Early stopping after {episode+1} episodes - no improvement for 30 episodes\")\n",
    "            if best_model is not None:\n",
    "                agent.policy.load_state_dict(best_model)\n",
    "            break\n",
    "                \n",
    "        # Evaluate agent periodically\n",
    "        if (episode + 1) % eval_freq == 0:\n",
    "            eval_reward = evaluate_episode(env, agent)\n",
    "            print(f\"Evaluation at episode {episode+1}: Reward = {eval_reward:.2f}\")\n",
    "    \n",
    "    # Update agent metrics\n",
    "    agent.rewards_history = episode_rewards\n",
    "    agent.losses = training_losses\n",
    "    agent.predictions = all_predictions\n",
    "    agent.true_states = all_true_states\n",
    "    \n",
    "    # If training completed without early stopping, load best model\n",
    "    if best_model is not None and episode == episodes - 1:\n",
    "        agent.policy.load_state_dict(best_model)\n",
    "        \n",
    "    # Calculate final accuracy\n",
    "    final_accuracy = np.mean(np.array(all_predictions) == np.array(all_true_states))\n",
    "    print(f\"Training completed. Final accuracy: {final_accuracy:.4f}\")\n",
    "    \n",
    "    return agent, episode_rewards, training_losses\n",
    "\n",
    "def evaluate_episode(env, agent):\n",
    "    \"\"\"Evaluate the agent on a single episode without exploration\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Initialize LSTM hidden state\n",
    "    hidden = None\n",
    "    state_sequence = deque(maxlen=agent.seq_len)\n",
    "    \n",
    "    # Run episode\n",
    "    while not done:\n",
    "        # Add current state to sequence buffer\n",
    "        state_sequence.append(state)\n",
    "        \n",
    "        # Pad sequence if needed\n",
    "        while len(state_sequence) < agent.seq_len:\n",
    "            state_sequence.appendleft(state)\n",
    "        \n",
    "        # Convert sequence to numpy array\n",
    "        state_seq = np.array(list(state_sequence))\n",
    "        \n",
    "        # Select action deterministically\n",
    "        action, hidden = agent.get_action(state_seq, hidden, evaluation=True)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        # Safety check\n",
    "        if steps >= 1000:\n",
    "            break\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def plot_training_results(rewards, losses=None, predictions=None, true_states=None):\n",
    "    \"\"\"Plot training metrics and model performance\"\"\"\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(rewards, label='Episode Reward')\n",
    "    plt.plot(pd.Series(rewards).rolling(10).mean(), 'r-', label='Moving Average (10)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot losses if available\n",
    "    if losses is not None and len(losses) > 0:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Update')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Plot accuracy if predictions and true states are provided\n",
    "    if predictions is not None and true_states is not None:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        # Calculate accuracy over time\n",
    "        window = 100\n",
    "        accuracies = []\n",
    "        for i in range(0, len(predictions), window):\n",
    "            end = min(i + window, len(predictions))\n",
    "            acc = np.mean(np.array(predictions[i:end]) == np.array(true_states[i:end]))\n",
    "            accuracies.append(acc)\n",
    "        \n",
    "        plt.plot(range(0, len(predictions), window), accuracies)\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Prediction Accuracy')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        plt.subplot(2, 2, 4)\n",
    "        cm = confusion_matrix(true_states, predictions)\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        classes = ['State 0', 'State 1']\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        # Add text annotations\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('True State')\n",
    "        plt.xlabel('Predicted State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def run_ppo_experiment(seed=42, episodes=300):\n",
    "    \"\"\"Run a complete experiment with the SBEOS environment and PPO agent\"\"\"\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = SBEOS_Environment(max_timesteps=500, window_size=25, time_dependence=4)\n",
    "    \n",
    "    # Get state and action dimensions\n",
    "    state_dim = len(env.generate_observation_state())\n",
    "    action_dim = 2  # Binary prediction: 0 or 1\n",
    "    \n",
    "    print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
    "    \n",
    "    # Create PPO agent\n",
    "    agent = PPOAgent(\n",
    "        state_size=state_dim,\n",
    "        action_size=action_dim,\n",
    "        hidden_dim=128,\n",
    "        learning_rate=0.0003,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        max_grad_norm=0.5,\n",
    "        ppo_epochs=4,\n",
    "        batch_size=64,\n",
    "        seq_len=8,\n",
    "        use_gae=True,\n",
    "    )\n",
    "    \n",
    "    # Train agent\n",
    "    trained_agent, rewards, losses = train_ppo_agent(\n",
    "        env, \n",
    "        agent, \n",
    "        episodes=episodes, \n",
    "        max_steps=500, \n",
    "        update_interval=1024, \n",
    "        eval_freq=20\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_results(\n",
    "        rewards, \n",
    "        losses=losses, \n",
    "        predictions=trained_agent.predictions, \n",
    "        true_states=trained_agent.true_states\n",
    "    )\n",
    "    \n",
    "    # Evaluate final performance\n",
    "    eval_rewards = []\n",
    "    for _ in range(20):  # Run 20 evaluation episodes\n",
    "        reward = evaluate_episode(env, trained_agent)\n",
    "        eval_rewards.append(reward)\n",
    "    \n",
    "    avg_reward = np.mean(eval_rewards)\n",
    "    print(f\"Final evaluation - Average reward over 20 episodes: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    trained_agent.save_model(\"ppo_lstm_model.pt\")\n",
    "    \n",
    "    return trained_agent\n",
    "\n",
    "def evaluate_on_test_environment(agent, test_env, num_episodes=100):\n",
    "    \"\"\"Evaluate agent performance on a separate test environment\"\"\"\n",
    "    test_rewards = []\n",
    "    test_predictions = []\n",
    "    test_true_states = []\n",
    "    \n",
    "    print(\"\\nEvaluating on test environment...\")\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state = test_env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        hidden = None\n",
    "        state_sequence = deque(maxlen=agent.seq_len)\n",
    "        \n",
    "        episode_predictions = []\n",
    "        episode_true_states = []\n",
    "        \n",
    "        # Run until episode ends\n",
    "        while not done:\n",
    "            # Add current state to sequence buffer\n",
    "            state_sequence.append(state)\n",
    "            \n",
    "            # Pad sequence if needed\n",
    "            while len(state_sequence) < agent.seq_len:\n",
    "                state_sequence.appendleft(state)\n",
    "            \n",
    "            # Convert sequence to numpy array\n",
    "            state_seq = np.array(list(state_sequence))\n",
    "            \n",
    "            # Select action deterministically\n",
    "            action, hidden = agent.get_action(state_seq, hidden, evaluation=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = test_env.step(action)\n",
    "            \n",
    "            # Record prediction and true state\n",
    "            episode_predictions.append(action)\n",
    "            episode_true_states.append(info[\"state\"])\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Store episode results\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_predictions.extend(episode_predictions)\n",
    "        test_true_states.extend(episode_true_states)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Test episode {i+1}/{num_episodes} completed. Reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_test_reward = np.mean(test_rewards)\n",
    "    test_accuracy = accuracy_score(test_true_states, test_predictions)\n",
    "    \n",
    "    print(f\"\\nTest Results Summary:\")\n",
    "    print(f\"Average reward: {avg_test_reward:.2f}\")\n",
    "    print(f\"Prediction accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': test_rewards,\n",
    "        'predictions': test_predictions,\n",
    "        'true_states': test_true_states,\n",
    "        'avg_reward': avg_test_reward,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "def create_test_environment(seed=None):\n",
    "    \"\"\"Create a separate test environment with a different random seed\"\"\"\n",
    "    # Use a different seed for the test environment to ensure different data generation\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the numpy random seed\n",
    "    \n",
    "    test_env = SBEOS_Environment(max_timesteps=500, window_size=25, time_dependence=4)\n",
    "    return test_env\n",
    "\n",
    "def plot_test_results(test_results):\n",
    "    \"\"\"Plot comprehensive test results including metrics and visualizations\"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot test rewards\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(test_results['rewards'])\n",
    "    plt.plot(pd.Series(test_results['rewards']).rolling(5).mean(), 'r-', label='Moving Average (5)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Test Rewards (Avg: {test_results[\"avg_reward\"]:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.subplot(2, 3, 2)\n",
    "    cm = confusion_matrix(test_results['true_states'], test_results['predictions'])\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix (Acc: {test_results[\"accuracy\"]:.4f})')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['State 0', 'State 1']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True State')\n",
    "    plt.xlabel('Predicted State')\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.subplot(2, 3, 3)\n",
    "    fpr, tpr, _ = roc_curve(test_results['true_states'], test_results['predictions'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot prediction pattern on a sample of test data\n",
    "    plt.subplot(2, 3, 4)\n",
    "    sample_size = min(500, len(test_results['predictions']))\n",
    "    plt.plot(test_results['true_states'][:sample_size], 'g-', label='True State', alpha=0.7)\n",
    "    plt.plot(test_results['predictions'][:sample_size], 'b--', label='Predicted State', alpha=0.7)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('State')\n",
    "    plt.title('Prediction Pattern (Sample)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "    plt.subplot(2, 3, 5)\n",
    "    precision, recall, _ = precision_recall_curve(test_results['true_states'], test_results['predictions'])\n",
    "    plt.plot(recall, precision, color='blue', lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot cumulative rewards\n",
    "    plt.subplot(2, 3, 6)\n",
    "    cumulative_rewards = np.cumsum(test_results['rewards'])\n",
    "    plt.plot(cumulative_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Test Rewards')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the complete experiment pipeline\"\"\"\n",
    "    print(\"Starting PPO agent experiment with LSTM policy for SBEOS environment\")\n",
    "    \n",
    "    # Set global random seed\n",
    "    global_seed = 42\n",
    "    torch.manual_seed(global_seed)\n",
    "    np.random.seed(global_seed)\n",
    "    random.seed(global_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(global_seed)\n",
    "    \n",
    "    # Run training\n",
    "    print(\"\\n=== Training Phase ===\")\n",
    "    trained_agent = run_ppo_experiment(seed=global_seed, episodes=500)\n",
    "    \n",
    "    # Create and evaluate on test environment\n",
    "    print(\"\\n=== Testing Phase ===\")\n",
    "    test_env = create_test_environment(seed=global_seed + 100)  # Different seed for test\n",
    "    test_results = evaluate_on_test_environment(trained_agent, test_env, num_episodes=100)\n",
    "    \n",
    "    # Plot test results\n",
    "    plot_test_results(test_results)\n",
    "    \n",
    "    # Save final model and results\n",
    "    trained_agent.save_model(\"final_ppo_lstm_model.pt\")\n",
    "    \n",
    "    # Save test metrics\n",
    "    test_metrics = {\n",
    "        'avg_reward': test_results['avg_reward'],\n",
    "        'accuracy': test_results['accuracy'],\n",
    "        'confusion_matrix': confusion_matrix(test_results['true_states'], test_results['predictions']).tolist(),\n",
    "    }\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(f\"Training episodes: 300\")\n",
    "    print(f\"Test episodes: 100\")\n",
    "    print(f\"Average test reward: {test_results['avg_reward']:.2f}\")\n",
    "    print(f\"Test accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(\"Experiment completed successfully!\")\n",
    "    \n",
    "    return trained_agent, test_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
