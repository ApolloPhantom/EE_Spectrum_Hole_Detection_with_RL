{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (799512573.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 45\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.show()import numpy as np\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_metrics_comparison(train_metrics, test_metrics):\n",
    "    \"\"\"Plot a comparison of key metrics between training and testing\"\"\"\n",
    "    # Calculate metrics for both datasets\n",
    "    train_true_states = []\n",
    "    train_predicted_states = []\n",
    "    \n",
    "    # Collect all states and predictions from training\n",
    "    for episode in range(len(train_metrics['episode_accuracies'])):\n",
    "        episode_index = episode * 100  # Assuming 100 timesteps per episode\n",
    "        if episode_index < len(train_metrics['all_rewards']):\n",
    "            train_true_states.append(1)  # Placeholder - we don't have actual train true states stored\n",
    "            train_predicted_states.append(1)  # Placeholder - we don't have actual train predictions stored\n",
    "    \n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    # We can only compare accuracy across train and test since we have limited metrics for training\n",
    "    train_values = [np.mean(train_metrics['episode_accuracies']), 0, 0, 0]\n",
    "    \n",
    "    # Get test metrics\n",
    "    test_performance = calculate_metrics(test_metrics)\n",
    "    test_values = [\n",
    "        test_performance['accuracy'],\n",
    "        test_performance['precision'],\n",
    "        test_performance['recall'],\n",
    "        test_performance['f1']\n",
    "    ]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_values, width, label='Training', alpha=0.7)\n",
    "    plt.bar(x + width/2, test_values, width, label='Testing', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Metrics Comparison')\n",
    "    plt.xticks(x, metrics_names)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from environments import SBEOS_Environment\n",
    "from tqdm import tqdm\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, \n",
    "                 exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        # We need to discretize the continuous state space first\n",
    "        self.n_bins = 10  # Number of bins for each dimension\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete state for Q-table lookup\"\"\"\n",
    "        # For simplicity, we'll use a basic binning approach\n",
    "        discretized = []\n",
    "        for value in state:\n",
    "            # Clip to reasonable range and discretize\n",
    "            bin_value = min(int(np.clip(value * self.n_bins, 0, self.n_bins - 1)), self.n_bins - 1)\n",
    "            discretized.append(bin_value)\n",
    "        return tuple(discretized)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        \n",
    "        if np.random.random() < self.exploration_rate:\n",
    "            # Explore: select random action\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            # Exploit: select best action from Q-table\n",
    "            if discretized_state not in self.q_table:\n",
    "                self.q_table[discretized_state] = np.zeros(self.action_size)\n",
    "            return np.argmax(self.q_table[discretized_state])\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-table using Q-learning update rule\"\"\"\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        discretized_next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        # Initialize Q-values for states if they don't exist\n",
    "        if discretized_state not in self.q_table:\n",
    "            self.q_table[discretized_state] = np.zeros(self.action_size)\n",
    "        if discretized_next_state not in self.q_table:\n",
    "            self.q_table[discretized_next_state] = np.zeros(self.action_size)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[discretized_next_state])\n",
    "        td_target = reward + (0 if done else self.discount_factor * self.q_table[discretized_next_state][best_next_action])\n",
    "        td_error = td_target - self.q_table[discretized_state][action]\n",
    "        \n",
    "        self.q_table[discretized_state][action] += self.learning_rate * td_error\n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.exploration_rate = max(self.min_exploration_rate, \n",
    "                                    self.exploration_rate * self.exploration_decay)\n",
    "\n",
    "def train_q_learning(env, episodes=1000):\n",
    "    \"\"\"Train Q-learning agent on SBEOS environment\"\"\"\n",
    "    # Initialize Q-learning agent\n",
    "    state_size = env.window_size + 6  # Based on observation space\n",
    "    action_size = 4  # 0: predict 0, 1: predict 1, 2: predict 0 with high energy, 3: predict 1 with high energy\n",
    "    \n",
    "    agent = QLearningAgent(state_size, action_size)\n",
    "    \n",
    "    # Track metrics\n",
    "    all_rewards = []\n",
    "    episode_rewards = []\n",
    "    episode_accuracies = []  # Track accuracy per episode\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # For tracking accuracy within each episode\n",
    "        episode_true_states = []\n",
    "        episode_predicted_states = []\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Store Q-values for ROC curve (difference between predicting 1 vs 0)\n",
    "            discretized_state = agent.discretize_state(state)\n",
    "            if discretized_state not in agent.q_table:\n",
    "                agent.q_table[discretized_state] = np.zeros(agent.action_size)\n",
    "            \n",
    "            # Simple case: compare Q-values for actions 0 and 1\n",
    "            action_value = agent.q_table[discretized_state][1] - agent.q_table[discretized_state][0]\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update_q_table(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_reward += reward\n",
    "            true_state = info[\"state\"]\n",
    "            predicted_state = action % 2  # Extract the binary prediction (0 or 1)\n",
    "            \n",
    "            episode_true_states.append(true_state)\n",
    "            episode_predicted_states.append(predicted_state)\n",
    "            all_rewards.append(reward)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate episode accuracy\n",
    "        if len(episode_true_states) > 0:  # Ensure we have predictions\n",
    "            episode_accuracy = accuracy_score(episode_true_states, episode_predicted_states)\n",
    "            episode_accuracies.append(episode_accuracy)\n",
    "        else:\n",
    "            episode_accuracies.append(0)\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_exploration_rate()\n",
    "        \n",
    "        # Track episode rewards\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward}, Accuracy: {episode_accuracies[-1]:.4f}, Exploration rate: {agent.exploration_rate:.4f}\")\n",
    "    \n",
    "    return agent, {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'all_rewards': all_rewards,\n",
    "        'episode_accuracies': episode_accuracies\n",
    "    }\n",
    "\n",
    "def test_q_learning(agent, episodes=100, seed=None):\n",
    "    \"\"\"Test trained Q-learning agent on a new environment instance with different seed\"\"\"\n",
    "    # Create a new environment instance\n",
    "    # Note: SBEOS_Environment doesn't accept a seed parameter\n",
    "    test_env = SBEOS_Environment(max_timesteps=100)\n",
    "    \n",
    "    # Use numpy's random state to ensure different randomization \n",
    "    if seed is not None:\n",
    "        # Set numpy's random state for this specific testing\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    # Track metrics\n",
    "    episode_rewards = []\n",
    "    episode_accuracies = []  # Track accuracy per episode\n",
    "    all_true_states = []\n",
    "    all_predicted_states = []\n",
    "    \n",
    "    # Testing loop\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = test_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # For tracking accuracy within each episode\n",
    "        episode_true_states = []\n",
    "        episode_predicted_states = []\n",
    "        \n",
    "        while not done:\n",
    "            # Select best action (no exploration)\n",
    "            discretized_state = agent.discretize_state(state)\n",
    "            if discretized_state not in agent.q_table:\n",
    "                agent.q_table[discretized_state] = np.zeros(agent.action_size)\n",
    "            \n",
    "            # Get Q-values for this state\n",
    "            q_values = agent.q_table[discretized_state]\n",
    "            action = np.argmax(q_values)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = test_env.step(action)\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_reward += reward\n",
    "            true_state = info[\"state\"]\n",
    "            predicted_state = action % 2  # Extract the binary prediction (0 or 1)\n",
    "            \n",
    "            episode_true_states.append(true_state)\n",
    "            episode_predicted_states.append(predicted_state)\n",
    "            all_true_states.append(true_state)\n",
    "            all_predicted_states.append(predicted_state)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate episode accuracy\n",
    "        if len(episode_true_states) > 0:  # Ensure we have predictions\n",
    "            episode_accuracy = accuracy_score(episode_true_states, episode_predicted_states)\n",
    "            episode_accuracies.append(episode_accuracy)\n",
    "        else:\n",
    "            episode_accuracies.append(0)\n",
    "        \n",
    "        # Track episode rewards\n",
    "        episode_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_accuracies': episode_accuracies,\n",
    "        'true_states': all_true_states,\n",
    "        'predicted_states': all_predicted_states\n",
    "    }\n",
    "\n",
    "def plot_rewards_and_accuracy(train_metrics, test_metrics):\n",
    "    \"\"\"Plot episode rewards and accuracy for training and testing\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot training rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_metrics['episode_rewards'])\n",
    "    plt.title('Training Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot testing rewards\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(test_metrics['episode_rewards'])\n",
    "    plt.title('Testing Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_metrics['episode_accuracies'])\n",
    "    plt.title('Training Accuracy per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot testing accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(test_metrics['episode_accuracies'])\n",
    "    plt.title('Testing Accuracy per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(test_metrics):\n",
    "    \"\"\"Plot confusion matrix for testing results\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Testing confusion matrix\n",
    "    test_cm = confusion_matrix(test_metrics['true_states'], test_metrics['predicted_states'])\n",
    "    sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title('Testing Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics(metrics):\n",
    "    \"\"\"Calculate overall accuracy, precision, recall and F1 score from true and predicted states\"\"\"\n",
    "    true_states = metrics['true_states']\n",
    "    predicted_states = metrics['predicted_states']\n",
    "    \n",
    "    accuracy = accuracy_score(true_states, predicted_states)\n",
    "    precision = precision_score(true_states, predicted_states, zero_division=0)\n",
    "    recall = recall_score(true_states, predicted_states, zero_division=0)\n",
    "    f1 = f1_score(true_states, predicted_states, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility for training\n",
    "    train_seed = 42\n",
    "    np.random.seed(train_seed)\n",
    "    \n",
    "    # Create training environment\n",
    "    print(\"Training on SBEOS environment...\")\n",
    "    train_env = SBEOS_Environment(max_timesteps=100)  # SBEOS_Environment doesn't accept seed parameter\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, train_metrics = train_q_learning(train_env, episodes=500)\n",
    "    \n",
    "    # Set a different seed for testing\n",
    "    test_seed = 84  # Different from training seed\n",
    "    \n",
    "    # Test the agent on an independent environment\n",
    "    print(\"Testing on independent SBEOS environment...\")\n",
    "    test_metrics = test_q_learning(agent, episodes=100, seed=test_seed)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_performance = calculate_metrics(test_metrics)\n",
    "    print(f\"Test Performance Metrics:\")\n",
    "    print(f\"  Accuracy:  {test_performance['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_performance['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {test_performance['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {test_performance['f1']:.4f}\")\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    plot_metrics_comparison(train_metrics, test_metrics)\n",
    "    \n",
    "    # Plot detailed metrics\n",
    "    plot_rewards_and_accuracy(train_metrics, test_metrics)\n",
    "    plot_confusion_matrix(test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
