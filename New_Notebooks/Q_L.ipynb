{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on single band environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:00<00:25, 19.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: 140, Exploration rate: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 104/500 [00:05<00:20, 19.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Reward: 500, Exploration rate: 0.6027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 205/500 [00:10<00:14, 19.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, Reward: 530, Exploration rate: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 303/500 [00:15<00:09, 19.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, Reward: 635, Exploration rate: 0.2212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 405/500 [00:20<00:04, 19.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, Reward: 650, Exploration rate: 0.1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 449/500 [00:23<00:02, 19.34it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from environments import SBEOS_Environment, MultiBandSBEOS\n",
    "from tqdm import tqdm\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, \n",
    "                 exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        # We need to discretize the continuous state space first\n",
    "        self.n_bins = 10  # Number of bins for each dimension\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete state for Q-table lookup\"\"\"\n",
    "        # For simplicity, we'll use a basic binning approach\n",
    "        discretized = []\n",
    "        for value in state:\n",
    "            # Clip to reasonable range and discretize\n",
    "            bin_value = min(int(np.clip(value * self.n_bins, 0, self.n_bins - 1)), self.n_bins - 1)\n",
    "            discretized.append(bin_value)\n",
    "        return tuple(discretized)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        \n",
    "        if np.random.random() < self.exploration_rate:\n",
    "            # Explore: select random action\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            # Exploit: select best action from Q-table\n",
    "            if discretized_state not in self.q_table:\n",
    "                self.q_table[discretized_state] = np.zeros(self.action_size)\n",
    "            return np.argmax(self.q_table[discretized_state])\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-table using Q-learning update rule\"\"\"\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        discretized_next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        # Initialize Q-values for states if they don't exist\n",
    "        if discretized_state not in self.q_table:\n",
    "            self.q_table[discretized_state] = np.zeros(self.action_size)\n",
    "        if discretized_next_state not in self.q_table:\n",
    "            self.q_table[discretized_next_state] = np.zeros(self.action_size)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[discretized_next_state])\n",
    "        td_target = reward + (0 if done else self.discount_factor * self.q_table[discretized_next_state][best_next_action])\n",
    "        td_error = td_target - self.q_table[discretized_state][action]\n",
    "        \n",
    "        self.q_table[discretized_state][action] += self.learning_rate * td_error\n",
    "        \n",
    "    def decay_exploration_rate(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.exploration_rate = max(self.min_exploration_rate, \n",
    "                                    self.exploration_rate * self.exploration_decay)\n",
    "\n",
    "def train_q_learning(env, episodes=1000, is_multiband=False):\n",
    "    \"\"\"Train Q-learning agent on SBEOS environment\"\"\"\n",
    "    # Initialize Q-learning agent\n",
    "    state_size = env.window_size + 6  # Based on observation space\n",
    "    \n",
    "    if is_multiband:\n",
    "        # For multiband, the observation includes features for each band\n",
    "        state_size = (env.window_size + 6) * env.num_bands\n",
    "        action_size = env.num_bands * 4  # 4 actions per band\n",
    "    else:\n",
    "        # For single band\n",
    "        action_size = 4  # 0: predict 0, 1: predict 1, 2: predict 0 with high energy, 3: predict 1 with high energy\n",
    "    \n",
    "    agent = QLearningAgent(state_size, action_size)\n",
    "    \n",
    "    # Track metrics\n",
    "    all_rewards = []\n",
    "    episode_rewards = []\n",
    "    true_states = []\n",
    "    predicted_states = []\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update_q_table(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_reward += reward\n",
    "            true_state = info[\"state\"]\n",
    "            predicted_state = action % 2  # Extract the binary prediction (0 or 1)\n",
    "            \n",
    "            true_states.append(true_state)\n",
    "            predicted_states.append(predicted_state)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_exploration_rate()\n",
    "        \n",
    "        # Track episode rewards\n",
    "        episode_rewards.append(episode_reward)\n",
    "        all_rewards.extend([reward] * env.max_timesteps)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward}, Exploration rate: {agent.exploration_rate:.4f}\")\n",
    "    \n",
    "    return agent, {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'all_rewards': all_rewards,\n",
    "        'true_states': true_states,\n",
    "        'predicted_states': predicted_states\n",
    "    }\n",
    "\n",
    "def test_q_learning(env, agent, episodes=100, is_multiband=False):\n",
    "    \"\"\"Test trained Q-learning agent\"\"\"\n",
    "    # Track metrics\n",
    "    episode_rewards = []\n",
    "    true_states = []\n",
    "    predicted_states = []\n",
    "    action_values = []  # Store action values for ROC curve\n",
    "    \n",
    "    # Testing loop\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select best action (no exploration)\n",
    "            discretized_state = agent.discretize_state(state)\n",
    "            if discretized_state not in agent.q_table:\n",
    "                agent.q_table[discretized_state] = np.zeros(agent.action_size)\n",
    "            \n",
    "            # Get Q-values for this state\n",
    "            q_values = agent.q_table[discretized_state]\n",
    "            action = np.argmax(q_values)\n",
    "            \n",
    "            # For ROC curve, get the difference between Q-values for predicting 0 vs 1\n",
    "            if is_multiband:\n",
    "                band_idx = action // 4\n",
    "                base_action_idx = action % 4\n",
    "                # Compare the max Q-value of predicting 0 vs predicting 1 for this band\n",
    "                action_0_idx = band_idx * 4 + 0\n",
    "                action_1_idx = band_idx * 4 + 1\n",
    "                if action_0_idx < len(q_values) and action_1_idx < len(q_values):\n",
    "                    action_value = q_values[action_1_idx] - q_values[action_0_idx]\n",
    "                else:\n",
    "                    action_value = 0\n",
    "            else:\n",
    "                # Compare Q-values for action 0 (predict 0) vs action 1 (predict 1)\n",
    "                action_value = q_values[1] - q_values[0]\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_reward += reward\n",
    "            true_state = info[\"state\"]\n",
    "            predicted_state = action % 2  # Extract the binary prediction (0 or 1)\n",
    "            \n",
    "            true_states.append(true_state)\n",
    "            predicted_states.append(predicted_state)\n",
    "            action_values.append(action_value)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Track episode rewards\n",
    "        episode_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'true_states': true_states,\n",
    "        'predicted_states': predicted_states,\n",
    "        'action_values': action_values\n",
    "    }\n",
    "\n",
    "def plot_rewards(train_metrics, test_metrics, title_prefix=\"\"):\n",
    "    \"\"\"Plot episode rewards for training and testing\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot training rewards\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_metrics['episode_rewards'])\n",
    "    plt.title(f'{title_prefix}Training Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot testing rewards\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_metrics['episode_rewards'])\n",
    "    plt.title(f'{title_prefix}Testing Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(train_metrics, test_metrics, title_prefix=\"\"):\n",
    "    \"\"\"Plot confusion matrices for training and testing\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Training confusion matrix\n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_cm = confusion_matrix(train_metrics['true_states'], train_metrics['predicted_states'])\n",
    "    sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'{title_prefix}Training Confusion Matrix')\n",
    "    \n",
    "    # Testing confusion matrix\n",
    "    plt.subplot(1, 2, 2)\n",
    "    test_cm = confusion_matrix(test_metrics['true_states'], test_metrics['predicted_states'])\n",
    "    sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'{title_prefix}Testing Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(train_metrics, test_metrics, title_prefix=\"\"):\n",
    "    \"\"\"Plot ROC curves for training and testing\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Training ROC curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_action_values = np.array(train_metrics['all_rewards'])[:len(train_metrics['true_states'])]\n",
    "    train_fpr, train_tpr, _ = roc_curve(train_metrics['true_states'], train_action_values)\n",
    "    train_roc_auc = auc(train_fpr, train_tpr)\n",
    "    plt.plot(train_fpr, train_tpr, label=f'ROC curve (area = {train_roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix}Training ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Testing ROC curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    test_fpr, test_tpr, _ = roc_curve(test_metrics['true_states'], test_metrics['action_values'])\n",
    "    test_roc_auc = auc(test_fpr, test_tpr)\n",
    "    plt.plot(test_fpr, test_tpr, label=f'ROC curve (area = {test_roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix}Testing ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create single band environment\n",
    "    print(\"Training on single band environment...\")\n",
    "    single_env = SBEOS_Environment(max_timesteps=100)\n",
    "    single_agent, single_train_metrics = train_q_learning(single_env, episodes=500)\n",
    "    single_test_metrics = test_q_learning(single_env, single_agent, episodes=100)\n",
    "    \n",
    "    # Plot metrics for single band\n",
    "    plot_rewards(single_train_metrics, single_test_metrics, \"Single Band: \")\n",
    "    plot_confusion_matrices(single_train_metrics, single_test_metrics, \"Single Band: \")\n",
    "    plot_roc_curves(single_train_metrics, single_test_metrics, \"Single Band: \")\n",
    "    \n",
    "    # Create multi-band environment\n",
    "    print(\"\\nTraining on multi-band environment...\")\n",
    "    multi_env = MultiBandSBEOS(num_bands=2, max_timesteps=100)\n",
    "    multi_agent, multi_train_metrics = train_q_learning(multi_env, episodes=500, is_multiband=True)\n",
    "    multi_test_metrics = test_q_learning(multi_env, multi_agent, episodes=100, is_multiband=True)\n",
    "    \n",
    "    # Plot metrics for multi-band\n",
    "    plot_rewards(multi_train_metrics, multi_test_metrics, \"Multi-Band: \")\n",
    "    plot_confusion_matrices(multi_train_metrics, multi_test_metrics, \"Multi-Band: \")\n",
    "    plot_roc_curves(multi_train_metrics, multi_test_metrics, \"Multi-Band: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paste'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpaste\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SBEOS_Environment, MultiBandSBEOS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msbeos_qlearning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_q_learning, test_q_learning, plot_rewards, plot_confusion_matrices, plot_roc_curves\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paste'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from environments import SBEOS_Environment, MultiBandSBEOS\n",
    "# from QLearningAgent import train_q_learning, test_q_learning, plot_rewards, plot_confusion_matrices, plot_roc_curves\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Parameters\n",
    "    training_episodes = 300  # Reduced for faster execution\n",
    "    testing_episodes = 50\n",
    "    timesteps = 120\n",
    "    \n",
    "    # Create output directory for plots\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    \n",
    "    print(\"Starting SBEOS Q-Learning experiments\")\n",
    "    \n",
    "    # Single band experiment\n",
    "    print(\"\\n==== Single Band SBEOS Environment ====\")\n",
    "    single_env = SBEOS_Environment(max_timesteps=timesteps)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Training Q-learning agent for {training_episodes} episodes...\")\n",
    "    single_agent, single_train_metrics = train_q_learning(\n",
    "        single_env, episodes=training_episodes, is_multiband=False\n",
    "    )\n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"Testing Q-learning agent for {testing_episodes} episodes...\")\n",
    "    single_test_metrics = test_q_learning(\n",
    "        single_env, single_agent, episodes=testing_episodes, is_multiband=False\n",
    "    )\n",
    "    \n",
    "    # Generate plots for single band\n",
    "    print(\"Generating single band plots...\")\n",
    "    \n",
    "    # Reward plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(single_train_metrics['episode_rewards'])\n",
    "    plt.title('Single Band: Training Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(single_test_metrics['episode_rewards'])\n",
    "    plt.title('Single Band: Testing Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/single_band_rewards.png\")\n",
    "    \n",
    "    # Confusion matrices\n",
    "    plot_confusion_matrices(single_train_metrics, single_test_metrics, \"Single Band: \")\n",
    "    plt.savefig(\"plots/single_band_confusion_matrices.png\")\n",
    "    \n",
    "    # ROC curves\n",
    "    plot_roc_curves(single_train_metrics, single_test_metrics, \"Single Band: \")\n",
    "    plt.savefig(\"plots/single_band_roc_curves.png\")\n",
    "    \n",
    "    # Multi-band experiment\n",
    "    print(\"\\n==== Multi-Band SBEOS Environment (2 bands) ====\")\n",
    "    multi_env = MultiBandSBEOS(num_bands=2, max_timesteps=timesteps)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Training Q-learning agent for {training_episodes} episodes...\")\n",
    "    multi_agent, multi_train_metrics = train_q_learning(\n",
    "        multi_env, episodes=training_episodes, is_multiband=True\n",
    "    )\n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"Testing Q-learning agent for {testing_episodes} episodes...\")\n",
    "    multi_test_metrics = test_q_learning(\n",
    "        multi_env, multi_agent, episodes=testing_episodes, is_multiband=True\n",
    "    )\n",
    "    \n",
    "    # Generate plots for multi band\n",
    "    print(\"Generating multi-band plots...\")\n",
    "    \n",
    "    # Reward plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(multi_train_metrics['episode_rewards'])\n",
    "    plt.title('Multi-Band: Training Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(multi_test_metrics['episode_rewards'])\n",
    "    plt.title('Multi-Band: Testing Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/multi_band_rewards.png\")\n",
    "    \n",
    "    # Confusion matrices\n",
    "    plot_confusion_matrices(multi_train_metrics, multi_test_metrics, \"Multi-Band: \")\n",
    "    plt.savefig(\"plots/multi_band_confusion_matrices.png\")\n",
    "    \n",
    "    # ROC curves\n",
    "    plot_roc_curves(multi_train_metrics, multi_test_metrics, \"Multi-Band: \")\n",
    "    plt.savefig(\"plots/multi_band_roc_curves.png\")\n",
    "    \n",
    "    print(\"\\nExperiments completed. Plots saved in 'plots' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
