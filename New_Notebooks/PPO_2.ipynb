{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting PPO agent experiment with LSTM policy for SBEOS environment\n",
      "\n",
      "=== Training Phase ===\n",
      "State dimension: 31, Action dimension: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training PPO with LSTM:   2%|▏         | 10/500 [00:57<50:49,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: 1205.00, Avg Reward (last 10): 1215.50, Accuracy: 0.4940, Steps: 500\n",
      "New best model saved! Avg reward: 1215.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   2%|▏         | 11/500 [01:03<50:26,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1232.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   3%|▎         | 13/500 [01:15<49:10,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1244.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   3%|▎         | 15/500 [01:29<53:26,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1245.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   3%|▎         | 16/500 [01:36<53:37,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1284.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   3%|▎         | 17/500 [01:43<56:06,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1292.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   4%|▎         | 18/500 [01:51<56:11,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1298.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   4%|▍         | 19/500 [01:58<56:32,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1311.50\n",
      "Episode: 20, Reward: 1040.00, Avg Reward (last 10): 1295.00, Accuracy: 0.4720, Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   4%|▍         | 20/500 [02:09<1:06:28,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 20: Reward = 1830.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   4%|▍         | 22/500 [02:21<57:01,  7.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1322.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   5%|▍         | 23/500 [02:27<53:55,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1334.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   5%|▌         | 25/500 [02:39<50:11,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1335.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   6%|▌         | 30/500 [03:09<46:49,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30, Reward: 1070.00, Avg Reward (last 10): 1236.50, Accuracy: 0.4760, Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO with LSTM:   8%|▊         | 38/500 [04:02<53:10,  6.91s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import copy\n",
    "import math\n",
    "from environments import SBEOS_Environment\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a transition tuple for PPO memory\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'reward', 'next_state', 'done', 'log_prob', 'value'))\n",
    "\n",
    "class PPOMemory:\n",
    "    \"\"\"Memory buffer for PPO algorithm\"\"\"\n",
    "    def __init__(self, capacity=2000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition to the memory buffer\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Return all transitions in the buffer\"\"\"\n",
    "        return Transition(*zip(*self.memory))\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the memory buffer\"\"\"\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    \"\"\"LSTM-based feature extractor for processing sequential data\"\"\"\n",
    "    def __init__(self, state_size, hidden_dim=64, num_layers=2):\n",
    "        super(LSTMFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Separate parts of the state\n",
    "        self.window_size = state_size - 6  # Raw window part of state\n",
    "        \n",
    "        # For processing the raw window\n",
    "        self.window_layer = nn.Sequential(\n",
    "            nn.Linear(self.window_size, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # For processing the statistical features\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(6, 16),  # 6 statistical features\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.pre_lstm_layer = nn.Sequential(\n",
    "            nn.Linear(32 + 16, 64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequence processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Handle different input shapes\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = 1 if x.dim() == 2 else x.size(1)\n",
    "        \n",
    "        if x.dim() == 2:  # [batch_size, features]\n",
    "            x = x.unsqueeze(1)  # Add sequence dimension\n",
    "            \n",
    "        # Reshape for processing\n",
    "        x_reshaped = x.view(batch_size * seq_len, -1)\n",
    "        \n",
    "        # Split input into raw window and statistical features\n",
    "        window = x_reshaped[:, :self.window_size]\n",
    "        features = x_reshaped[:, self.window_size:]\n",
    "        \n",
    "        # Process through respective paths\n",
    "        window_features = self.window_layer(window)\n",
    "        stat_features = self.feature_layer(features)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([window_features, stat_features], dim=1)\n",
    "        pre_lstm = self.pre_lstm_layer(combined)\n",
    "        \n",
    "        # Reshape back to [batch_size, seq_len, features]\n",
    "        pre_lstm = pre_lstm.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        if hidden is None:\n",
    "            lstm_out, hidden = self.lstm(pre_lstm)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(pre_lstm, hidden)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Return the final output for each sequence\n",
    "        return lstm_out, hidden\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Combined Actor-Critic network with LSTM backbone\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Feature extraction with LSTM\n",
    "        self.feature_extractor = LSTMFeatureExtractor(state_size, hidden_dim=hidden_dim)\n",
    "        \n",
    "        # Policy network (Actor)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, action_size),\n",
    "        )\n",
    "        \n",
    "        # Value network (Critic)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Extract features\n",
    "        features, hidden = self.feature_extractor(x, hidden)\n",
    "        \n",
    "        # Get last sequence outputs if sequence data\n",
    "        if features.dim() > 2:\n",
    "            features = features[:, -1]\n",
    "        \n",
    "        # Calculate policy and value\n",
    "        action_probs = F.softmax(self.actor(features), dim=-1)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_probs, value, hidden\n",
    "    \n",
    "    def get_action(self, state, hidden=None, evaluation=False):\n",
    "        \"\"\"Select an action based on the policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action_probs, value, hidden = self.forward(state, hidden)\n",
    "            \n",
    "            if evaluation:\n",
    "                # Deterministic action for evaluation\n",
    "                action = torch.argmax(action_probs, dim=-1)\n",
    "            else:\n",
    "                # Stochastic action for training\n",
    "                dist = Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "            \n",
    "        if evaluation:\n",
    "            return action[0].item(), hidden\n",
    "        else:\n",
    "            return action[0].item(), log_prob[0], value[0].squeeze(), hidden\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        hidden_dim=128,\n",
    "        learning_rate=0.0003,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        max_grad_norm=0.5,\n",
    "        ppo_epochs=4,\n",
    "        batch_size=64,\n",
    "        seq_len=8,\n",
    "        use_gae=True,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gae = use_gae\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Initialize the actor-critic network\n",
    "        self.policy = ActorCritic(state_size, action_size, hidden_dim=hidden_dim).to(device)\n",
    "        \n",
    "        # Initialize optimizer with learning rate scheduler\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=10, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Initialize memory buffer\n",
    "        self.memory = PPOMemory(capacity=2000)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.rewards_history = []\n",
    "        self.losses = []\n",
    "        self.entropy_history = []\n",
    "        self.predictions = []\n",
    "        self.true_states = []\n",
    "        \n",
    "        # Sequence buffer for LSTM states\n",
    "        self.state_buffer = []\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        \"\"\"Store a transition in memory\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done, log_prob, value)\n",
    "    \n",
    "    def get_action(self, state, hidden=None, evaluation=False):\n",
    "        \"\"\"Select an action using the policy\"\"\"\n",
    "        # Convert state to tensor\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor([state]).to(device)\n",
    "            \n",
    "        # Make sure it has batch dimension\n",
    "        if state_tensor.dim() == 1:\n",
    "            state_tensor = state_tensor.unsqueeze(0)\n",
    "        \n",
    "        return self.policy.get_action(state_tensor, hidden, evaluation)\n",
    "    \n",
    "    def compute_gae(self, rewards, values, next_value, dones):\n",
    "        \"\"\"Compute returns and advantages using Generalized Advantage Estimation (GAE)\"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        # Start from the last timestep and work backwards\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            # For the last state, use next_value as bootstrap\n",
    "            if t == len(rewards) - 1:\n",
    "                # Delta = rt + γ*V(st+1) - V(st)\n",
    "                next_non_terminal = 1.0 - dones[t]\n",
    "                next_val = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[t + 1]\n",
    "                next_val = values[t + 1]\n",
    "            \n",
    "            # Calculate TD error: δt = rt + γ*V(st+1) - V(st)\n",
    "            delta = rewards[t] + self.gamma * next_non_terminal * next_val - values[t]\n",
    "            \n",
    "            # Calculate GAE: At = δt + γ*λ*At+1\n",
    "            gae = delta + self.gamma * self.gae_lambda * next_non_terminal * gae\n",
    "            \n",
    "            # Insert at the beginning of the list (since we're going backwards)\n",
    "            advantages.insert(0, gae)\n",
    "            \n",
    "            # Calculate returns: Gt = At + V(st)\n",
    "            returns.insert(0, gae + values[t])\n",
    "            \n",
    "        return returns, advantages\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy using PPO algorithm\"\"\"\n",
    "        # Check if we have enough transitions\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get all transitions from memory\n",
    "        transitions = self.memory.sample()\n",
    "        \n",
    "        # Convert transitions to tensors\n",
    "        states = torch.FloatTensor(np.array(transitions.state)).to(device)\n",
    "        actions = torch.LongTensor(transitions.action).to(device)\n",
    "        rewards = torch.FloatTensor(transitions.reward).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(transitions.next_state)).to(device)\n",
    "        dones = torch.FloatTensor(transitions.done).to(device)\n",
    "        old_log_probs = torch.FloatTensor(transitions.log_prob).to(device)\n",
    "        old_values = torch.FloatTensor(transitions.value).to(device)\n",
    "        \n",
    "        # Get the last state value to bootstrap returns\n",
    "        with torch.no_grad():\n",
    "            _, next_value, _ = self.policy(states[-1:])\n",
    "            next_value = next_value.squeeze()\n",
    "        \n",
    "        # Compute returns and advantages using GAE\n",
    "        returns, advantages = self.compute_gae(\n",
    "            rewards.cpu().numpy(), \n",
    "            old_values.cpu().numpy(), \n",
    "            next_value.cpu().numpy(), \n",
    "            dones.cpu().numpy()\n",
    "        )\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        \n",
    "        # Normalize advantages for training stability\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        total_loss = 0\n",
    "        actor_loss_total = 0\n",
    "        critic_loss_total = 0\n",
    "        entropy_total = 0\n",
    "        \n",
    "        # Split into batches\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # Generate random indices for minibatches\n",
    "            indices = np.random.permutation(len(states))\n",
    "            \n",
    "            # Process in batches\n",
    "            for start_idx in range(0, len(states), self.batch_size):\n",
    "                # Get minibatch indices\n",
    "                batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "                \n",
    "                # Get minibatch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                \n",
    "                # Reset hidden state for each batch\n",
    "                hidden = None\n",
    "                \n",
    "                # Get current policy outputs\n",
    "                action_probs, current_values, _ = self.policy(batch_states, hidden)\n",
    "                current_values = current_values.squeeze()\n",
    "                \n",
    "                # Calculate log probs\n",
    "                dist = Categorical(action_probs)\n",
    "                current_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Calculate ratios and clipped objective\n",
    "                ratios = torch.exp(current_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss with clipping\n",
    "                value_pred_clipped = batch_returns + torch.clamp(\n",
    "                    current_values - batch_returns, -self.clip_ratio, self.clip_ratio\n",
    "                )\n",
    "                value_loss_1 = F.mse_loss(current_values, batch_returns)\n",
    "                value_loss_2 = F.mse_loss(value_pred_clipped, batch_returns)\n",
    "                value_loss = torch.max(value_loss_1, value_loss_2)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = actor_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Optimize the model\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                actor_loss_total += actor_loss.item()\n",
    "                critic_loss_total += value_loss.item()\n",
    "                entropy_total += entropy.item()\n",
    "        \n",
    "        # Clear memory after update\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Return average losses\n",
    "        num_updates = self.ppo_epochs * ((len(states) + self.batch_size - 1) // self.batch_size)\n",
    "        return {\n",
    "            'total_loss': total_loss / num_updates,\n",
    "            'actor_loss': actor_loss_total / num_updates,\n",
    "            'critic_loss': critic_loss_total / num_updates,\n",
    "            'entropy': entropy_total / num_updates\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filepath)\n",
    "        \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def train_ppo_agent(env, agent, episodes=500, max_steps=500, update_interval=512, eval_freq=10):\n",
    "    \"\"\"Train the PPO agent\"\"\"\n",
    "    # Record best model for early stopping\n",
    "    best_reward = float('-inf')\n",
    "    best_model = None\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    all_predictions = []\n",
    "    all_true_states = []\n",
    "    training_losses = []\n",
    "    entropy_history = []\n",
    "    \n",
    "    # Episode window for tracking improvement\n",
    "    window_size = 10\n",
    "    window_rewards = deque(maxlen=window_size)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(episodes), desc=\"Training PPO with LSTM\"):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_losses = []\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        # Initialize LSTM hidden state\n",
    "        hidden = None\n",
    "        state_sequence = deque(maxlen=agent.seq_len)\n",
    "        \n",
    "        # Collect episode data\n",
    "        episode_predictions = []\n",
    "        episode_true_states = []\n",
    "        \n",
    "        # Step counter for PPO updates\n",
    "        step_count = 0\n",
    "        \n",
    "        # Run episode\n",
    "        while not done and steps < max_steps:\n",
    "            # Add current state to sequence buffer\n",
    "            state_sequence.append(state)\n",
    "            \n",
    "            # Pad sequence if needed\n",
    "            while len(state_sequence) < agent.seq_len:\n",
    "                state_sequence.appendleft(state)\n",
    "            \n",
    "            # Convert sequence to numpy array\n",
    "            state_seq = np.array(list(state_sequence))\n",
    "            \n",
    "            # Select action\n",
    "            action, log_prob, value, hidden = agent.get_action(state_seq, hidden)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Record prediction and true state\n",
    "            episode_predictions.append(action)\n",
    "            episode_true_states.append(info[\"state\"])\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done, log_prob, value)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            step_count += 1\n",
    "            \n",
    "            # Update policy if enough steps have been accumulated\n",
    "            if step_count >= update_interval or done:\n",
    "                if len(agent.memory) > agent.batch_size:\n",
    "                    loss_info = agent.update_policy()\n",
    "                    episode_losses.append(loss_info['total_loss'])\n",
    "                    step_count = 0\n",
    "        \n",
    "        # Record episode metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        all_predictions.extend(episode_predictions)\n",
    "        all_true_states.extend(episode_true_states)\n",
    "        window_rewards.append(episode_reward)\n",
    "        \n",
    "        # Record losses if any updates happened\n",
    "        if episode_losses:\n",
    "            training_losses.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Calculate accuracy for the episode\n",
    "        episode_accuracy = np.mean(np.array(episode_predictions) == np.array(episode_true_states))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            window_avg = np.mean(window_rewards)\n",
    "            print(f\"Episode: {episode+1}, Reward: {episode_reward:.2f}, Avg Reward (last {window_size}): {window_avg:.2f}, \"\n",
    "                  f\"Accuracy: {episode_accuracy:.4f}, Steps: {steps}\")\n",
    "            \n",
    "            # Update learning rate based on performance\n",
    "            agent.scheduler.step(window_avg)\n",
    "        \n",
    "        # Save best model with more sophisticated early stopping\n",
    "        if len(window_rewards) == window_size:\n",
    "            window_avg = np.mean(window_rewards)\n",
    "            if window_avg > best_reward + 0.1:  # Require significant improvement\n",
    "                best_reward = window_avg\n",
    "                best_model = copy.deepcopy(agent.policy.state_dict())\n",
    "                no_improvement_count = 0\n",
    "                print(f\"New best model saved! Avg reward: {best_reward:.2f}\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "            \n",
    "            # Adaptive early stopping - require more evidence before stopping for higher rewards\n",
    "            patience = min(30, max(15, int(50 - best_reward)))\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f\"Early stopping after {episode+1} episodes - no improvement for {patience} episodes\")\n",
    "                if best_model is not None:\n",
    "                    agent.policy.load_state_dict(best_model)\n",
    "                break\n",
    "                \n",
    "        # Evaluate agent periodically\n",
    "        if (episode + 1) % eval_freq == 0:\n",
    "            eval_reward = evaluate_episode(env, agent)\n",
    "            print(f\"Evaluation at episode {episode+1}: Reward = {eval_reward:.2f}\")\n",
    "    \n",
    "    # Update agent metrics\n",
    "    agent.rewards_history = episode_rewards\n",
    "    agent.losses = training_losses\n",
    "    agent.predictions = all_predictions\n",
    "    agent.true_states = all_true_states\n",
    "    \n",
    "    # If training completed without early stopping, load best model\n",
    "    if best_model is not None and episode == episodes - 1:\n",
    "        agent.policy.load_state_dict(best_model)\n",
    "        \n",
    "    # Calculate final accuracy\n",
    "    final_accuracy = np.mean(np.array(all_predictions) == np.array(all_true_states))\n",
    "    print(f\"Training completed. Final accuracy: {final_accuracy:.4f}, Best average reward: {best_reward:.2f}\")\n",
    "    \n",
    "    return agent, episode_rewards, training_losses\n",
    "\n",
    "def evaluate_episode(env, agent):\n",
    "    \"\"\"Evaluate the agent on a single episode without exploration\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Initialize LSTM hidden state\n",
    "    hidden = None\n",
    "    state_sequence = deque(maxlen=agent.seq_len)\n",
    "    \n",
    "    # Run episode\n",
    "    while not done and steps < 1000:  # Add step limit for safety\n",
    "        # Add current state to sequence buffer\n",
    "        state_sequence.append(state)\n",
    "        \n",
    "        # Pad sequence if needed\n",
    "        while len(state_sequence) < agent.seq_len:\n",
    "            state_sequence.appendleft(state)\n",
    "        \n",
    "        # Convert sequence to numpy array\n",
    "        state_seq = np.array(list(state_sequence))\n",
    "        \n",
    "        # Select action deterministically\n",
    "        action, hidden = agent.get_action(state_seq, hidden, evaluation=True)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def plot_training_results(rewards, losses=None, predictions=None, true_states=None):\n",
    "    \"\"\"Plot training metrics and model performance\"\"\"\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(rewards, label='Episode Reward')\n",
    "    plt.plot(pd.Series(rewards).rolling(10).mean(), 'r-', label='Moving Average (10)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot losses if available\n",
    "    if losses is not None and len(losses) > 0:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Update')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Plot accuracy if predictions and true states are provided\n",
    "    if predictions is not None and true_states is not None:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        # Calculate accuracy over time\n",
    "        window = 100\n",
    "        accuracies = []\n",
    "        for i in range(0, len(predictions), window):\n",
    "            end = min(i + window, len(predictions))\n",
    "            acc = np.mean(np.array(predictions[i:end]) == np.array(true_states[i:end]))\n",
    "            accuracies.append(acc)\n",
    "        \n",
    "        plt.plot(range(0, len(predictions), window), accuracies)\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Prediction Accuracy')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        plt.subplot(2, 2, 4)\n",
    "        cm = confusion_matrix(true_states, predictions)\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        classes = ['State 0', 'State 1']\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        # Add text annotations\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('True State')\n",
    "        plt.xlabel('Predicted State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def run_ppo_experiment(seed=42, episodes=300):\n",
    "    \"\"\"Run a complete experiment with the SBEOS environment and PPO agent\"\"\"\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = SBEOS_Environment(max_timesteps=500, window_size=25, time_dependence=4)\n",
    "    \n",
    "    # Get state and action dimensions\n",
    "    state_dim = len(env.generate_observation_state())\n",
    "    action_dim = 2  # Binary prediction: 0 or 1\n",
    "    \n",
    "    print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
    "    \n",
    "    # Create PPO agent with optimized hyperparameters\n",
    "    agent = PPOAgent(\n",
    "        state_size=state_dim,\n",
    "        action_size=action_dim,\n",
    "        hidden_dim=128,\n",
    "        learning_rate=0.0003,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,  # Slightly higher entropy for more exploration\n",
    "        max_grad_norm=0.5,\n",
    "        ppo_epochs=4,\n",
    "        batch_size=64,\n",
    "        seq_len=8,\n",
    "        use_gae=True,\n",
    "    )\n",
    "    \n",
    "    # Train agent with smaller update interval (512 instead of 1024)\n",
    "    trained_agent, rewards, losses = train_ppo_agent(\n",
    "        env, \n",
    "        agent, \n",
    "        episodes=episodes, \n",
    "        max_steps=500, \n",
    "        update_interval=512,  # Reduced update interval for faster learning\n",
    "        eval_freq=20\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plot = plot_training_results(\n",
    "        rewards, \n",
    "        losses=losses, \n",
    "        predictions=trained_agent.predictions, \n",
    "        true_states=trained_agent.true_states\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate final performance\n",
    "    eval_rewards = []\n",
    "    for _ in range(20):  # Run 20 evaluation episodes\n",
    "        reward = evaluate_episode(env, trained_agent)\n",
    "        eval_rewards.append(reward)\n",
    "    \n",
    "    avg_reward = np.mean(eval_rewards)\n",
    "    print(f\"Final evaluation - Average reward over 20 episodes: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    trained_agent.save_model(\"ppo_lstm_model.pt\")\n",
    "    \n",
    "    return trained_agent\n",
    "\n",
    "def evaluate_on_test_environment(agent, test_env, num_episodes=100):\n",
    "    \"\"\"Evaluate agent performance on a separate test environment\"\"\"\n",
    "    test_rewards = []\n",
    "    test_predictions = []\n",
    "    test_true_states = []\n",
    "    \n",
    "    print(\"\\nEvaluating on test environment...\")\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state = test_env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        hidden = None\n",
    "        state_sequence = deque(maxlen=agent.seq_len)\n",
    "        \n",
    "        episode_predictions = []\n",
    "        episode_true_states = []\n",
    "        \n",
    "        # Run until episode ends or max steps\n",
    "        while not done and steps < 1000:\n",
    "            # Add current state to sequence buffer\n",
    "            state_sequence.append(state)\n",
    "            \n",
    "            # Pad sequence if needed\n",
    "            while len(state_sequence) < agent.seq_len:\n",
    "                state_sequence.appendleft(state)\n",
    "            \n",
    "            # Convert sequence to numpy array\n",
    "            state_seq = np.array(list(state_sequence))\n",
    "            \n",
    "            # Select action deterministically\n",
    "            action, hidden = agent.get_action(state_seq, hidden, evaluation=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = test_env.step(action)\n",
    "            \n",
    "            # Record prediction and true state\n",
    "            episode_predictions.append(action)\n",
    "            episode_true_states.append(info[\"state\"])\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Store episode results\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_predictions.extend(episode_predictions)\n",
    "        test_true_states.extend(episode_true_states)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Test episode {i+1}/{num_episodes} completed. Reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_test_reward = np.mean(test_rewards)\n",
    "    test_accuracy = accuracy_score(test_true_states, test_predictions)\n",
    "    \n",
    "    print(f\"\\nTest Results Summary:\")\n",
    "    print(f\"Average reward: {avg_test_reward:.2f}\")\n",
    "    print(f\"Prediction accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': test_rewards,\n",
    "        'predictions': test_predictions,\n",
    "        'true_states': test_true_states,\n",
    "        'avg_reward': avg_test_reward,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "def create_test_environment(seed=None):\n",
    "    \"\"\"Create a separate test environment with a different random seed\"\"\"\n",
    "    # Use a different seed for the test environment to ensure different data generation\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the numpy random seed\n",
    "    \n",
    "    test_env = SBEOS_Environment(max_timesteps=500, window_size=25, time_dependence=4)\n",
    "    return test_env\n",
    "\n",
    "def plot_test_results(test_results):\n",
    "    \"\"\"Plot comprehensive test results including metrics and visualizations\"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot test rewards\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(test_results['rewards'])\n",
    "    plt.plot(pd.Series(test_results['rewards']).rolling(5).mean(), 'r-', label='Moving Average (5)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Test Rewards (Avg: {test_results[\"avg_reward\"]:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.subplot(2, 3, 2)\n",
    "    cm = confusion_matrix(test_results['true_states'], test_results['predictions'])\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix (Acc: {test_results[\"accuracy\"]:.4f})')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['State 0', 'State 1']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True State')\n",
    "    plt.xlabel('Predicted State')\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.subplot(2, 3, 3)\n",
    "    fpr, tpr, _ = roc_curve(test_results['true_states'], test_results['predictions'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot prediction pattern on a sample of test data\n",
    "    plt.subplot(2, 3, 4)\n",
    "    sample_size = min(500, len(test_results['predictions']))\n",
    "    plt.plot(test_results['true_states'][:sample_size], 'g-', label='True State', alpha=0.7)\n",
    "    plt.plot(test_results['predictions'][:sample_size], 'b--', label='Predicted State', alpha=0.7)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('State')\n",
    "    plt.title('Prediction Pattern (Sample)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "    plt.subplot(2, 3, 5)\n",
    "    precision, recall, _ = precision_recall_curve(test_results['true_states'], test_results['predictions'])\n",
    "    plt.plot(recall, precision, color='blue', lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot cumulative rewards\n",
    "    plt.subplot(2, 3, 6)\n",
    "    cumulative_rewards = np.cumsum(test_results['rewards'])\n",
    "    plt.plot(cumulative_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Test Rewards')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the complete experiment pipeline\"\"\"\n",
    "    print(\"Starting PPO agent experiment with LSTM policy for SBEOS environment\")\n",
    "    \n",
    "    # Set global random seed\n",
    "    global_seed = 42\n",
    "    torch.manual_seed(global_seed)\n",
    "    np.random.seed(global_seed)\n",
    "    random.seed(global_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(global_seed)\n",
    "    \n",
    "    # Run training\n",
    "    print(\"\\n=== Training Phase ===\")\n",
    "    trained_agent = run_ppo_experiment(seed=global_seed, episodes=500)\n",
    "    \n",
    "    # Create and evaluate on test environment\n",
    "    print(\"\\n=== Testing Phase ===\")\n",
    "    test_env = create_test_environment(seed=global_seed + 100)  # Different seed for test\n",
    "    test_results = evaluate_on_test_environment(trained_agent, test_env, num_episodes=100)\n",
    "    \n",
    "    # Plot test results\n",
    "    plot_test_results(test_results)\n",
    "    \n",
    "    # Save final model and results\n",
    "    trained_agent.save_model(\"final_ppo_lstm_model.pt\")\n",
    "    \n",
    "    # Save test metrics\n",
    "    test_metrics = {\n",
    "        'avg_reward': test_results['avg_reward'],\n",
    "        'accuracy': test_results['accuracy'],\n",
    "        'confusion_matrix': confusion_matrix(test_results['true_states'], test_results['predictions']).tolist(),\n",
    "    }\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(f\"Training episodes: 300\")\n",
    "    print(f\"Test episodes: 100\")\n",
    "    print(f\"Average test reward: {test_results['avg_reward']:.2f}\")\n",
    "    print(f\"Test accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(\"Experiment completed successfully!\")\n",
    "    \n",
    "    return trained_agent, test_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
