{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class ReinforcementEnvironment:\n",
    "    def __init__(self, num_bands, energy_cost=2, reward_factor=5, weight=5, max_timestep=180):\n",
    "        self.num_bands = num_bands\n",
    "        self.energy_cost = energy_cost\n",
    "        self.reward_factor = reward_factor\n",
    "        self.max_timestep = max_timestep\n",
    "        self.weight = weight\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.transition_matrixes = {band: {} for band in range(self.num_bands)}\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "    \n",
    "    def init_bands(self):\n",
    "        \"\"\"Initialize each band with two initial signal values (0 or 1)\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # First signal chosen with equal probability\n",
    "            t1 = np.random.choice([0, 1])\n",
    "            \n",
    "            # Second signal chosen with random probability distribution\n",
    "            t_m1 = np.random.rand(2,2)\n",
    "            t_m1 /= t_m1.sum(axis=1,keepdims=True)  # Normalize to create valid probability distribution\n",
    "            t2 = np.random.choice([0, 1], p=t_m1[t1])\n",
    "            # t_m2 = {\n",
    "            #     (0, 0): np.random.rand(2),\n",
    "            #     (0, 1): np.random.rand(2),\n",
    "            #     (1, 0): np.random.rand(2),\n",
    "            #     (1, 1): np.random.rand(2)\n",
    "            # }\n",
    "            # for k in t_m2:\n",
    "            #     t_m2[k] /= t_m2[k].sum()\n",
    "            t_m2 = {\n",
    "            (0, 0): np.random.dirichlet([1, 1]),  # Generates a valid probability distribution over {0,1}\n",
    "            (0, 1): np.random.dirichlet([1, 1]),\n",
    "            (1, 0): np.random.dirichlet([1, 1]),\n",
    "            (1, 1): np.random.dirichlet([1, 1])\n",
    "            }\n",
    "            self.transition_matrixes[band] = t_m2\n",
    "            self.signal_band[band] = [t1, t2]  \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one time step within the environment\n",
    "        \n",
    "        Args:\n",
    "            action: tuple (band, prediction) where band is the selected frequency band\n",
    "                   and prediction is the predicted signal value (0 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (observation, reward, done, info)\n",
    "        \"\"\"\n",
    "        self.current_timestep += 1\n",
    "        \n",
    "        band = action[0]\n",
    "        prediction = action[1]\n",
    "        \n",
    "        reward = self._calculate_reward(self.current_state[band], prediction)\n",
    "        \n",
    "        self.generate_state()\n",
    "        \n",
    "        observation = self.construct_observation_space()\n",
    "        \n",
    "        done = self.current_timestep >= self.max_timestep\n",
    "        \n",
    "        info = {\n",
    "            \"timestep\": self.current_timestep,\n",
    "            \"correct_prediction\": self.current_state[band] == prediction,\n",
    "            \"state\": self.current_state\n",
    "        }\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self, actual_signal, prediction):\n",
    "        \"\"\"Calculate reward based on prediction accuracy and signal value\"\"\"\n",
    "        if actual_signal == prediction:\n",
    "            # Correct prediction\n",
    "            reward = self.reward_factor * self.weight - self.energy_cost\n",
    "        elif actual_signal == 0:\n",
    "            # Incorrect prediction when signal is 0\n",
    "            reward = self.reward_factor - self.energy_cost\n",
    "        else:  # actual_signal == 1\n",
    "            # Incorrect prediction when signal is 1\n",
    "            reward = self.reward_factor - self.energy_cost * self.weight\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def generate_state(self):\n",
    "        \"\"\"Generate next state for all bands based on transition probabilities\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # Get last two signals for this band\n",
    "            p_2 = tuple(self.signal_band[band][-2:])\n",
    "            \n",
    "            t_m2 = self.transition_matrixes[band]\n",
    "            \n",
    "            next_signal = np.random.choice([0, 1], p=t_m2[p_2])\n",
    "            \n",
    "            self.signal_band[band].append(next_signal)\n",
    "            self.signal_band[band].pop(0)\n",
    "        \n",
    "        # Update current state\n",
    "        self.current_state = self.get_current_state()\n",
    "        \n",
    "        return self.current_state\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Return the current state as a list of the most recent signal for each band\"\"\"\n",
    "        return [self.signal_band[band][-1] for band in range(self.num_bands)]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state and return initial observation\"\"\"\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "        return self.construct_observation_space()\n",
    "    \n",
    "    def construct_observation_space(self, window_size=10):\n",
    "        \"\"\"\n",
    "        Construct observation space with entropy calculations for each band\n",
    "        \n",
    "        Args:\n",
    "            window_size: Number of recent signals to consider for entropy calculation\n",
    "            \n",
    "        Returns:\n",
    "            list: Entropy values for each band\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "        for band in range(self.num_bands):\n",
    "            signal_values = np.array(self.signal_band[band][-window_size:])\n",
    "            \n",
    "            if len(signal_values) <= window_size:\n",
    "                entropy_value = 0\n",
    "            else:\n",
    "                value_counts = np.bincount(signal_values, minlength=2)\n",
    "                \n",
    "                probability_distribution = value_counts / len(signal_values)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if np.all(probability_distribution == 0):\n",
    "                    entropy_value = 0\n",
    "                else:\n",
    "                    # Calculate entropy using scipy function\n",
    "                    entropy_value = entropy(probability_distribution, base=2)\n",
    "            \n",
    "            observation.append(entropy_value)\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def soft_reset(self):\n",
    "        self.signal_band = {band: self.signal_band[band][-2:] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.generate_state()\n",
    "        return self.construct_observation_space()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "class DuelingDeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Dueling Deep Q-Network Architecture\n",
    "        \n",
    "        Separates value estimation into:\n",
    "        1. State value function (V)\n",
    "        2. Advantage function (A)\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input observation space\n",
    "            output_dim (int): Number of possible actions\n",
    "        \"\"\"\n",
    "        super(DuelingDeepQNetwork, self).__init__()\n",
    "        \n",
    "        # Shared feature layers\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Single value output\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)  # One output per action\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure input is float tensor and has correct shape\n",
    "        x = x.float()\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        # Extract shared features\n",
    "        features = self.feature_layer(x)\n",
    "        \n",
    "        # Compute value and advantage streams\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        \n",
    "        # Combine value and advantage streams\n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "        action_values = values + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        \n",
    "        return action_values\n",
    "\n",
    "class DuelingDQNAgent:\n",
    "    def __init__(self, env, input_dim, output_dim, exploration_type='epsilon'):\n",
    "        \"\"\"\n",
    "        Dueling DQN Agent with flexible exploration strategies\n",
    "        \n",
    "        Args:\n",
    "            env: Environment\n",
    "            input_dim: Input dimension\n",
    "            output_dim: Output dimension\n",
    "            exploration_type: 'epsilon' or 'softmax'\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.exploration_type = exploration_type\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DuelingDeepQNetwork(input_dim, output_dim).to(self.device)\n",
    "        self.target_network = DuelingDeepQNetwork(input_dim, output_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.0005\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        \n",
    "        # Exploration parameters\n",
    "        if exploration_type == 'epsilon':\n",
    "            self.epsilon = 1.0\n",
    "            self.epsilon_min = 0.01\n",
    "            self.epsilon_decay = 0.9975\n",
    "            self.epsilon_decay_steps = 500\n",
    "        else:  # softmax\n",
    "            self.temperature = 1.0\n",
    "            self.min_temperature = 0.1\n",
    "            self.temperature_decay = 0.9995\n",
    "            self.temperature_decay_steps = 500\n",
    "        \n",
    "        # Replay memory\n",
    "        self.replay_memory = []\n",
    "        self.memory_size = 20000\n",
    "        self.batch_size = 128\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.q_network.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = F.smooth_l1_loss\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Action selection with flexible exploration strategies\n",
    "        \"\"\"\n",
    "        if self.exploration_type == 'epsilon':\n",
    "            # Epsilon-greedy exploration\n",
    "            self.epsilon = max(\n",
    "                self.epsilon_min, \n",
    "                self.epsilon * (self.epsilon_decay ** (1 / self.epsilon_decay_steps))\n",
    "            )\n",
    "            \n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                # Random exploration\n",
    "                band = np.random.randint(0, len(state))\n",
    "                prediction = np.random.randint(0, 2)\n",
    "                return (band, prediction)\n",
    "        else:  # softmax\n",
    "            # Softmax exploration\n",
    "            self.temperature = max(\n",
    "                self.min_temperature, \n",
    "                self.temperature * (self.temperature_decay ** (1 / self.temperature_decay_steps))\n",
    "            )\n",
    "        \n",
    "        # Exploitation phase (common for both exploration types)\n",
    "        with torch.no_grad():\n",
    "            # Prepare state tensor\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "            \n",
    "            # Get Q-values from the network\n",
    "            q_values = self.q_network(state_tensor).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            if self.exploration_type == 'softmax':\n",
    "                # Compute softmax probabilities with temperature scaling\n",
    "                scaled_q_values = q_values / self.temperature\n",
    "                scaled_q_values -= np.max(scaled_q_values)\n",
    "                \n",
    "                exp_q = np.exp(scaled_q_values)\n",
    "                action_probs = exp_q / np.sum(exp_q)\n",
    "                \n",
    "                # Sample action based on softmax probabilities\n",
    "                action_idx = np.random.choice(len(q_values), p=action_probs)\n",
    "            else:\n",
    "                # Greedy selection for epsilon strategy\n",
    "                action_idx = np.argmax(q_values)\n",
    "            \n",
    "            # Convert action index to (band, prediction)\n",
    "            band = action_idx // 2\n",
    "            prediction = action_idx % 2\n",
    "            \n",
    "            return (band, prediction)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store transition in replay memory\n",
    "        \"\"\"\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.replay_memory) < self.memory_size:\n",
    "            self.replay_memory.append(experience)\n",
    "        else:\n",
    "            # Replace random experience if memory is full\n",
    "            idx = random.randint(0, len(self.replay_memory) - 1)\n",
    "            self.replay_memory[idx] = experience\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Experience replay with Double DQN approach\n",
    "        \"\"\"\n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)\n",
    "        \n",
    "        # Prepare batch tensors\n",
    "        states = torch.tensor(np.array([b[0] for b in batch]), dtype=torch.float32).to(self.device)\n",
    "        actions = [b[1] for b in batch]\n",
    "        rewards = torch.tensor(np.array([b[2] for b in batch]), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array([b[3] for b in batch]), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(np.array([b[4] for b in batch]), dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.q_network(states)\n",
    "        \n",
    "        # Double DQN: select actions from main network, evaluate from target\n",
    "        next_q_values_main = self.q_network(next_states)\n",
    "        next_q_values_target = self.target_network(next_states)\n",
    "        \n",
    "        max_next_actions = next_q_values_main.argmax(1)\n",
    "        max_next_q_values = next_q_values_target.gather(1, max_next_actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "        \n",
    "        # Select current Q-values for taken actions\n",
    "        batch_actions = [action[0] * 2 + action[1] for action in actions]\n",
    "        current_q_values = current_q_values.gather(1, torch.tensor(batch_actions, dtype=torch.long).unsqueeze(1).to(self.device)).squeeze(1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q_values, target_q_values.detach())\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, episodes=1000):\n",
    "        \"\"\"\n",
    "        Training loop with comprehensive metrics\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "        self.env.reset()\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            # Reset environment\n",
    "            state = self.env.soft_reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                total_predictions += 1\n",
    "                if info['correct_prediction']:\n",
    "                    correct_predictions += 1\n",
    "                \n",
    "                self.store_transition(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                self.experience_replay()\n",
    "            \n",
    "            # Periodic target network update\n",
    "            if episode % 100 == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "            \n",
    "            # Logging\n",
    "            if episode % 50 == 0:\n",
    "                accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "                print(f\"Episode {episode}, \"\n",
    "                      f\"Total Reward: {total_reward:.2f}, \"\n",
    "                      f\"Accuracy: {accuracy:.2%}, \"\n",
    "                      f\"{self.exploration_type.capitalize()} Value: {getattr(self, self.exploration_type):.4f}\")\n",
    "        \n",
    "        return total_rewards\n",
    "    \n",
    "def plot_rewards(rewards, window_size=10):\n",
    "        plt.figure(figsize=(12, 6), facecolor='white')\n",
    "        plt.plot(rewards, alpha=0.5, color='lightblue', label='Episode Reward')\n",
    "        moving_average = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(np.arange(window_size-1, len(rewards)), moving_average, color='blue', linewidth=2, label=f'{window_size}-Episode Moving Avg')\n",
    "        plt.title('Training Reward over Episodes', fontweight='bold')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "\n",
    "\n",
    "def main():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    num_bands = 10\n",
    "    env = ReinforcementEnvironment(num_bands)\n",
    "    \n",
    "    input_dim = num_bands\n",
    "    output_dim = num_bands * 2\n",
    "    \n",
    "    # Epsilon-greedy agent\n",
    "    epsilon_agent = DuelingDQNAgent(env, input_dim, output_dim, exploration_type='epsilon')\n",
    "    epsilon_rewards = epsilon_agent.train(episodes=500)\n",
    "    \n",
    "    # Softmax agent\n",
    "    softmax_agent = DuelingDQNAgent(env, input_dim, output_dim, exploration_type='softmax')\n",
    "    softmax_rewards = softmax_agent.train(episodes=500)\n",
    "    \n",
    "    # Plotting rewards would be done here\n",
    "    plot_rewards(epsilon_rewards, title='Dueling DQN with Epsilon Exploration')\n",
    "    plot_rewards(softmax_rewards, title='Dueling DQN with Softmax Exploration')\n",
    "\n",
    "# Assuming plot_rewards function is defined similarly to previous implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 2172.00, Accuracy: 52.22%, Epsilon Value: 0.9991\n",
      "Episode 50, Total Reward: 2280.00, Accuracy: 51.67%, Epsilon Value: 0.9551\n",
      "Episode 100, Total Reward: 2028.00, Accuracy: 57.78%, Epsilon Value: 0.9130\n",
      "Episode 150, Total Reward: 2264.00, Accuracy: 56.11%, Epsilon Value: 0.8728\n",
      "Episode 200, Total Reward: 2336.00, Accuracy: 60.00%, Epsilon Value: 0.8343\n",
      "Episode 250, Total Reward: 1972.00, Accuracy: 57.78%, Epsilon Value: 0.7976\n",
      "Episode 300, Total Reward: 2164.00, Accuracy: 58.89%, Epsilon Value: 0.7624\n",
      "Episode 350, Total Reward: 2584.00, Accuracy: 58.33%, Epsilon Value: 0.7288\n",
      "Episode 400, Total Reward: 2396.00, Accuracy: 67.22%, Epsilon Value: 0.6967\n",
      "Episode 450, Total Reward: 2616.00, Accuracy: 68.89%, Epsilon Value: 0.6660\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DuelingDQNAgent' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 307\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Softmax agent\u001b[39;00m\n\u001b[1;32m    306\u001b[0m softmax_agent \u001b[38;5;241m=\u001b[39m DuelingDQNAgent(env, input_dim, output_dim, exploration_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 307\u001b[0m softmax_rewards \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Plotting rewards would be done here\u001b[39;00m\n\u001b[1;32m    310\u001b[0m plot_rewards(epsilon_rewards, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDueling DQN with Epsilon Exploration\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 272\u001b[0m, in \u001b[0;36mDuelingDQNAgent.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    268\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions \u001b[38;5;28;01mif\u001b[39;00m total_predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 272\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_type\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_type)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_rewards\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DuelingDQNAgent' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
