{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import copy\n",
    "import math\n",
    "from environments import SBEOS_Environment\n",
    "\n",
    "time_dependence = 4\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a transition tuple for experience replay\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'priority'))\n",
    "\n",
    "# Add standard ReplayBuffer class that was missing\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Standard experience replay buffer\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        # Add a dummy priority of 1.0 to match the Transition namedtuple\n",
    "        self.memory.append(Transition(*args, priority=1.0))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions randomly\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return []\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized experience replay buffer for more efficient learning\"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha             # How much prioritization to use (0 = no prioritization, 1 = full prioritization)\n",
    "        self.beta = beta_start         # Importance sampling correction factor\n",
    "        self.beta_frames = beta_frames # Number of frames over which to anneal beta to 1\n",
    "        self.frame = 1                 # Current frame counter\n",
    "        self.epsilon = 1e-6            # Small constant to ensure all priorities > 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition with max priority on first insertion\"\"\"\n",
    "        # If memory is not empty, use max priority; otherwise use 1.0\n",
    "        max_priority = max([t.priority for t in self.memory]) if self.memory else 1.0\n",
    "        self.memory.append(Transition(*args, priority=max_priority))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions with prioritization\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return [], [], []\n",
    "        \n",
    "        # Calculate selection probabilities based on priorities\n",
    "        priorities = np.array([t.priority for t in self.memory])\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Update beta\n",
    "        self.beta = min(1.0, self.beta + self.frame * (1.0 - self.beta) / self.beta_frames)\n",
    "        self.frame += 1\n",
    "        \n",
    "        # Sample using the probabilities\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs, replace=False)\n",
    "        samples = [self.memory[idx] for idx in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        weights = (len(self.memory) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()  # Normalize weights\n",
    "        weights = torch.FloatTensor(weights).to(device)\n",
    "        \n",
    "        return samples, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities for sampled transitions\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            if 0 <= idx < len(self.memory):  # Ensure index is valid\n",
    "                self.memory[idx] = self.memory[idx]._replace(priority=priority.item() + self.epsilon)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Dueling DQN architecture with:\n",
    "    - Separate processing paths for binary and continuous features\n",
    "    - Temporal learning for binary features\n",
    "    - Skip connections\n",
    "    - Attention mechanisms\n",
    "    - Layer normalization for improved stability\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_dim=256):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        # Split state dimensions\n",
    "        self.binary_size = state_size - 6  # First n-6 states are binary\n",
    "        self.continuous_size = 6           # Last 6 states are continuous\n",
    "        \n",
    "        # Binary features processing path (with temporal learning)\n",
    "        self.binary_lstm = nn.LSTM(\n",
    "            input_size=1,                   # Process each binary feature as a sequence\n",
    "            hidden_size=hidden_dim // 4,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Binary features post-processing\n",
    "        self.binary_path = nn.Sequential(\n",
    "            nn.Linear(self.binary_size * (hidden_dim // 4), hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Continuous features processing path\n",
    "        self.continuous_path = nn.Sequential(\n",
    "            nn.Linear(self.continuous_size, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Feature fusion layer with attention\n",
    "        self.fusion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Residual/skip connection processing\n",
    "        self.residual_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream with residual connections\n",
    "        self.value_stream1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.value_stream2 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value_output = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "        # Advantage stream with residual connections\n",
    "        self.advantage_stream1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.advantage_stream2 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.advantage_output = nn.Linear(hidden_dim // 2, action_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights with He initialization for ReLU activations\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Split input into binary and continuous parts\n",
    "        binary_input = x[:, :self.binary_size]\n",
    "        continuous_input = x[:, -self.continuous_size:]\n",
    "        \n",
    "        # Process binary features with temporal learning (LSTM)\n",
    "        # Reshape binary input to (batch_size, binary_size, 1) for LSTM\n",
    "        binary_input = binary_input.unsqueeze(-1)\n",
    "        binary_features, _ = self.binary_lstm(binary_input)\n",
    "        # Flatten the output: combine all temporal features\n",
    "        binary_features = binary_features.reshape(batch_size, -1)\n",
    "        binary_features = self.binary_path(binary_features)\n",
    "        \n",
    "        # Process continuous features\n",
    "        continuous_features = self.continuous_path(continuous_input)\n",
    "        \n",
    "        # Combine features with attention mechanism\n",
    "        # Reshape for attention: [batch_size, seq_len=1, hidden_dim]\n",
    "        binary_attn = binary_features.unsqueeze(1)\n",
    "        continuous_attn = continuous_features.unsqueeze(1)\n",
    "        \n",
    "        # Self-attention on concatenated features\n",
    "        combined_features = torch.cat([binary_attn, continuous_attn], dim=1)\n",
    "        attn_output, _ = self.fusion_attention(combined_features, combined_features, combined_features)\n",
    "        \n",
    "        # Mean pooling over sequence dimension to get feature vector\n",
    "        fused_features = attn_output.mean(dim=1)\n",
    "        \n",
    "        # Skip connection: Original features + attention-processed features\n",
    "        combined_raw = torch.cat([binary_features, continuous_features], dim=1)\n",
    "        features = self.residual_layer(combined_raw) + fused_features\n",
    "        \n",
    "        # Value stream with residual connection\n",
    "        value_features = self.value_stream1(features)\n",
    "        value_residual = self.value_stream2(value_features)\n",
    "        value = self.value_output(value_features + value_residual)\n",
    "        \n",
    "        # Advantage stream with residual connection\n",
    "        advantage_features = self.advantage_stream1(features)\n",
    "        advantage_residual = self.advantage_stream2(advantage_features)\n",
    "        advantages = self.advantage_output(advantage_features + advantage_residual)\n",
    "        \n",
    "        # Dueling architecture formula with improved numerical stability\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"\n",
    "        Method to extract learned features for analysis or visualization\n",
    "        Returns the processed features before the dueling architecture split\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Split input into binary and continuous parts\n",
    "        binary_input = x[:, :self.binary_size]\n",
    "        continuous_input = x[:, -self.continuous_size:]\n",
    "        \n",
    "        # Process binary features with temporal learning\n",
    "        binary_input = binary_input.unsqueeze(-1)\n",
    "        binary_features, _ = self.binary_lstm(binary_input)\n",
    "        binary_features = binary_features.reshape(batch_size, -1)\n",
    "        binary_features = self.binary_path(binary_features)\n",
    "        \n",
    "        # Process continuous features\n",
    "        continuous_features = self.continuous_path(continuous_input)\n",
    "        \n",
    "        # Attention fusion\n",
    "        binary_attn = binary_features.unsqueeze(1)\n",
    "        continuous_attn = continuous_features.unsqueeze(1)\n",
    "        combined_features = torch.cat([binary_attn, continuous_attn], dim=1)\n",
    "        attn_output, attn_weights = self.fusion_attention(\n",
    "            combined_features, combined_features, combined_features, \n",
    "            need_weights=True\n",
    "        )\n",
    "        \n",
    "        # Return all intermediate representations for analysis\n",
    "        return {\n",
    "            'binary_features': binary_features,\n",
    "            'continuous_features': continuous_features,\n",
    "            'attention_weights': attn_weights,\n",
    "            'fused_features': attn_output.mean(dim=1)\n",
    "        }\n",
    "\n",
    "class ImprovedDuelingDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        memory_size=50000,\n",
    "        batch_size=128,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=50000,\n",
    "        learning_rate=0.0001,\n",
    "        target_update_freq=1000,   # Hard update frequency (steps)\n",
    "        use_soft_update=True,      # Whether to use soft updates\n",
    "        tau=0.005,                 # Soft update parameter\n",
    "        double_dqn=True,           # Whether to use Double DQN algorithm\n",
    "        prioritized_replay=True,   # Whether to use prioritized experience replay\n",
    "        n_step=3,                  # N-step returns\n",
    "        reward_clip=None,          # Optional reward clipping\n",
    "        adam_eps=1e-8              # Adam epsilon for numerical stability\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.use_soft_update = use_soft_update\n",
    "        self.tau = tau\n",
    "        self.double_dqn = double_dqn\n",
    "        self.prioritized_replay = prioritized_replay\n",
    "        self.n_step = n_step\n",
    "        self.reward_clip = reward_clip\n",
    "        self.steps_done = 0\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # N-step transition buffer\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        \n",
    "        # Initialize replay buffer\n",
    "        if prioritized_replay:\n",
    "            self.memory = PrioritizedReplayBuffer(memory_size)\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(memory_size)\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.policy_net = DuelingDQN(state_size, action_size).to(device)\n",
    "        self.target_net = DuelingDQN(state_size, action_size).to(device)\n",
    "        \n",
    "        # Initialize target network with policy network weights\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Set target network to evaluation mode\n",
    "        \n",
    "        # Optimizer with weight decay for regularization and improved numerical stability\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.policy_net.parameters(), \n",
    "            lr=learning_rate,\n",
    "            eps=adam_eps,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler for dynamic adjustment\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            mode='max', \n",
    "            factor=0.5, \n",
    "            patience=10,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training metrics\n",
    "        self.rewards_history = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.predictions = []\n",
    "        self.true_states = []\n",
    "        \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"Select an action using epsilon-greedy policy\"\"\"\n",
    "        # Epsilon-greedy action selection\n",
    "        if not evaluate and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Convert state to tensor\n",
    "        with torch.no_grad():\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state_tensor = torch.FloatTensor(state).to(device)\n",
    "            else:\n",
    "                state_tensor = torch.FloatTensor([state]).to(device)\n",
    "                \n",
    "            # Make sure it has batch dimension\n",
    "            if state_tensor.dim() == 1:\n",
    "                state_tensor = state_tensor.unsqueeze(0)\n",
    "            \n",
    "            # Set policy network to eval mode to avoid BatchNorm errors with single samples\n",
    "            was_training = self.policy_net.training\n",
    "            self.policy_net.eval()\n",
    "            \n",
    "            # Get Q-values and select best action\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            \n",
    "            # Restore previous training state\n",
    "            if was_training:\n",
    "                self.policy_net.train()\n",
    "                \n",
    "            return q_values.argmax(1).item()\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Update epsilon using exponential decay\"\"\"\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon - self.epsilon_end) * \\\n",
    "                       math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in n-step buffer\"\"\"\n",
    "        # Add to n-step buffer\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # If buffer is full enough, compute n-step return and store in replay memory\n",
    "        if len(self.n_step_buffer) == self.n_step:\n",
    "            state, action, n_reward, next_state, done = self._get_n_step_info()\n",
    "            \n",
    "            # Apply reward clipping if specified\n",
    "            if self.reward_clip is not None:\n",
    "                n_reward = max(min(n_reward, self.reward_clip), -self.reward_clip)\n",
    "                \n",
    "            # Store in replay memory\n",
    "            self.memory.push(state, action, next_state, n_reward, done)\n",
    "        \n",
    "        # If episode is over, empty the n-step buffer and add remaining transitions\n",
    "        if done:\n",
    "            while len(self.n_step_buffer) > 0:\n",
    "                state, action, n_reward, next_state, done = self._get_n_step_info()\n",
    "                \n",
    "                # Apply reward clipping if specified\n",
    "                if self.reward_clip is not None:\n",
    "                    n_reward = max(min(n_reward, self.reward_clip), -self.reward_clip)\n",
    "                    \n",
    "                # Store in replay memory\n",
    "                self.memory.push(state, action, next_state, n_reward, done)\n",
    "    \n",
    "    def _get_n_step_info(self):\n",
    "        \"\"\"Calculate n-step returns and prepare transition for replay buffer\"\"\"\n",
    "        # Get first stored transition\n",
    "        state, action, reward, next_state, done = self.n_step_buffer[0]\n",
    "        \n",
    "        # Calculate n-step return\n",
    "        for idx in range(1, len(self.n_step_buffer)):\n",
    "            r, n_s, d = self.n_step_buffer[idx][2:5]\n",
    "            reward += (self.gamma ** idx) * r\n",
    "            if d:\n",
    "                done = True\n",
    "                next_state = n_s\n",
    "                break\n",
    "        \n",
    "        # Remove the first transition from buffer\n",
    "        self.n_step_buffer.popleft()\n",
    "        \n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train the model with experiences from replay buffer\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        if self.prioritized_replay:\n",
    "            transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "            if not transitions:  # Empty list check\n",
    "                return 0\n",
    "        else:\n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "            if not transitions:  # Empty list check\n",
    "                return 0\n",
    "            weights = torch.ones(self.batch_size).to(device)\n",
    "            indices = None\n",
    "        \n",
    "        # Convert batch to tensors\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(np.array(batch.state)).to(device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Ensure batch size is sufficient for BatchNorm\n",
    "        if len(state_batch) <= 1:\n",
    "            return 0  # Skip update if batch size is too small\n",
    "        \n",
    "        # Compute current Q values\n",
    "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Compute next Q values based on algorithm choice\n",
    "        with torch.no_grad():\n",
    "            # Set target network to eval mode (important for BatchNorm)\n",
    "            self.target_net.eval()\n",
    "            \n",
    "            if self.double_dqn:\n",
    "                # For Double DQN, set policy network to eval mode temporarily\n",
    "                self.policy_net.eval()\n",
    "                next_actions = self.policy_net(next_state_batch).max(1, keepdim=True)[1]\n",
    "                self.policy_net.train()  # Restore to training mode\n",
    "                \n",
    "                next_q_values = self.target_net(next_state_batch).gather(1, next_actions)\n",
    "            else:\n",
    "                # Standard DQN: Use max Q-value from target network\n",
    "                next_q_values = self.target_net(next_state_batch).max(1, keepdim=True)[0]\n",
    "            \n",
    "            # Compute expected Q values\n",
    "            expected_q_values = reward_batch + self.gamma ** self.n_step * next_q_values * (1 - done_batch)\n",
    "        \n",
    "        # Compute loss (Huber loss for stability)\n",
    "        elementwise_loss = F.smooth_l1_loss(current_q_values, expected_q_values, reduction='none')\n",
    "        \n",
    "        # Apply importance sampling weights for prioritized replay\n",
    "        loss = (elementwise_loss * weights).mean()\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities for prioritized replay\n",
    "        if self.prioritized_replay and indices is not None:\n",
    "            # TD error as priority\n",
    "            td_errors = (expected_q_values - current_q_values).abs()\n",
    "            self.memory.update_priorities(indices, td_errors)\n",
    "        \n",
    "        # Update target network if needed\n",
    "        self.update_counter += 1\n",
    "        if self.use_soft_update:\n",
    "            # Soft update every step\n",
    "            self.soft_update_target_network()\n",
    "        elif self.update_counter % self.target_update_freq == 0:\n",
    "            # Hard update periodically\n",
    "            self.hard_update_target_network()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def hard_update_target_network(self):\n",
    "        \"\"\"Hard update target network weights\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def soft_update_target_network(self):\n",
    "        \"\"\"Soft update target network weights\"\"\"\n",
    "        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * policy_param.data + (1.0 - self.tau) * target_param.data)\n",
    "    \n",
    "    def update_learning_rate(self, reward):\n",
    "        \"\"\"Update learning rate based on performance\"\"\"\n",
    "        self.scheduler.step(reward)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps_done': self.steps_done\n",
    "        }, filepath)\n",
    "        \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.target_net.eval()\n",
    "\n",
    "\n",
    "def evaluate_episode(env, agent):\n",
    "    \"\"\"Evaluate the agent on a single episode without exploration\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Select action without exploration\n",
    "        action = agent.select_action(state, evaluate=True)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def train_improved_dueling_dqn(env, agent, episodes=500, eval_freq=10, log_freq=10):\n",
    "    \"\"\"Train the improved dueling DQN agent\"\"\"\n",
    "    # Record best model for early stopping\n",
    "    best_reward = float('-inf')\n",
    "    best_model = None\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    all_predictions = []\n",
    "    all_true_states = []\n",
    "    training_losses = []\n",
    "    \n",
    "    # Episode window for tracking improvement\n",
    "    window_size = 10\n",
    "    window_rewards = deque(maxlen=window_size)\n",
    "    \n",
    "    # For recording improvement trends\n",
    "    recent_losses = deque(maxlen=100)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(episodes), desc=\"Training Improved Dueling DQN\"):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        loss_count = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        # Episode experience collection\n",
    "        episode_predictions = []\n",
    "        episode_true_states = []\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Record prediction and true state\n",
    "            episode_predictions.append(action)\n",
    "            episode_true_states.append(info[\"state\"])\n",
    "            \n",
    "            # Store transition - this handles n-step returns internally\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "                \n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Train the network every 4 steps for stability\n",
    "            if steps % 4 == 0:\n",
    "                loss = agent.replay()\n",
    "                if loss is not None:\n",
    "                    episode_loss += loss\n",
    "                    loss_count += 1\n",
    "                    recent_losses.append(loss)\n",
    "            \n",
    "            # Update epsilon for exploration\n",
    "            agent.update_epsilon()\n",
    "        \n",
    "        # Additional training at end of episode\n",
    "        loss = agent.replay()\n",
    "        if loss is not None:\n",
    "            episode_loss += loss\n",
    "            loss_count += 1\n",
    "            recent_losses.append(loss)\n",
    "        \n",
    "        # Record episode metrics\n",
    "        avg_loss = episode_loss / max(1, loss_count)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        all_predictions.extend(episode_predictions)\n",
    "        all_true_states.extend(episode_true_states)\n",
    "        training_losses.append(avg_loss)\n",
    "        window_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calculate accuracy for the episode\n",
    "        episode_accuracy = np.mean(np.array(episode_predictions) == np.array(episode_true_states))\n",
    "        \n",
    "        # Update learning rate based on performance\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            agent.update_learning_rate(np.mean(window_rewards))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % log_freq == 0:\n",
    "            window_avg = np.mean(window_rewards)\n",
    "            recent_loss_avg = np.mean(recent_losses) if recent_losses else 0\n",
    "            print(f\"Episode: {episode+1}, Reward: {episode_reward:.2f}, Avg Reward (last {window_size}): {window_avg:.2f}, \"\n",
    "                  f\"Accuracy: {episode_accuracy:.4f}, Recent Loss: {recent_loss_avg:.6f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "            \n",
    "        # Save best model\n",
    "        if len(window_rewards) == window_size:\n",
    "            window_avg = np.mean(window_rewards)\n",
    "            if window_avg > best_reward:\n",
    "                best_reward = window_avg\n",
    "                best_model = copy.deepcopy(agent.policy_net.state_dict())\n",
    "                no_improvement_count = 0\n",
    "                print(f\"New best model saved! Avg reward: {best_reward:.2f}\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "        \n",
    "        # Early stopping if no improvement for a while, but with a higher threshold\n",
    "        if no_improvement_count >= 50:\n",
    "            print(f\"Early stopping after {episode+1} episodes - no improvement for 50 episodes\")\n",
    "            if best_model is not None:\n",
    "                agent.policy_net.load_state_dict(best_model)\n",
    "            break\n",
    "                \n",
    "        # Evaluate agent periodically\n",
    "        if (episode + 1) % eval_freq == 0:\n",
    "            eval_reward = evaluate_episode(env, agent)\n",
    "            print(f\"Evaluation at episode {episode+1}: Reward = {eval_reward:.2f}\")\n",
    "            \n",
    "    # Update agent metrics\n",
    "    agent.rewards_history = episode_rewards\n",
    "    agent.losses = training_losses\n",
    "    agent.predictions = all_predictions\n",
    "    agent.true_states = all_true_states\n",
    "    \n",
    "    # If training completed without early stopping, load best model\n",
    "    if best_model is not None and episode == episodes - 1:\n",
    "        agent.policy_net.load_state_dict(best_model)\n",
    "        \n",
    "    # Calculate final accuracy\n",
    "    final_accuracy = np.mean(np.array(all_predictions) == np.array(all_true_states))\n",
    "    print(f\"Training completed. Final accuracy: {final_accuracy:.4f}\")\n",
    "    \n",
    "    return agent, episode_rewards, training_losses\n",
    "\n",
    "\n",
    "def plot_training_results(rewards, losses, predictions=None, true_states=None):\n",
    "    \"\"\"Plot training metrics and model performance\"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(rewards, label='Episode Reward')\n",
    "    plt.plot(pd.Series(rewards).rolling(10).mean(), 'r-', label='Moving Average (10)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot losses with smoother rolling average\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(losses, alpha=0.5, label='Raw Loss')\n",
    "    plt.plot(pd.Series(losses).rolling(20).mean(), 'r-', label='Moving Average (20)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')  # Log scale often helps visualize loss better\n",
    "    \n",
    "    # Plot accuracy if predictions and true states are provided\n",
    "    if predictions is not None and true_states is not None:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        # Calculate accuracy over time with smaller window for finer granularity\n",
    "        window = 100\n",
    "        steps = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for i in range(0, len(predictions), window):\n",
    "            end = min(i + window, len(predictions))\n",
    "            acc = np.mean(np.array(predictions[i:end]) == np.array(true_states[i:end]))\n",
    "            accuracies.append(acc)\n",
    "            steps.append(i)\n",
    "        \n",
    "        plt.plot(steps, accuracies)\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Prediction Accuracy Over Time')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        plt.subplot(2, 2, 4)\n",
    "        cm = confusion_matrix(true_states, predictions)\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=\"Blues\")\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        classes = ['State 0', 'State 1']\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        # Add text annotations\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('True State')\n",
    "        plt.xlabel('Predicted State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_improved_experiment(seed=42, episodes=300):\n",
    "    \"\"\"Run a complete experiment with the SBEOS environment and improved dueling DQN agent\"\"\"\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = SBEOS_Environment(max_timesteps=200, window_size=20)\n",
    "    \n",
    "    # Get state and action dimensions\n",
    "    state_dim = len(env.generate_observation_state())\n",
    "    action_dim = 2  # Binary prediction: 0 or 1\n",
    "    \n",
    "    # Create improved agent\n",
    "    agent = ImprovedDuelingDQNAgent(\n",
    "        state_size=state_dim,\n",
    "        action_size=action_dim,\n",
    "        memory_size=100000,          # Larger memory for better experience diversity\n",
    "        batch_size=128,              # Larger batch size for stable gradients\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=50000,         # Slower epsilon decay\n",
    "        learning_rate=0.0001,        # Lower learning rate for stability\n",
    "        target_update_freq=1000,     # Hard update frequency (steps)\n",
    "        use_soft_update=True,        # Use soft updates for stability\n",
    "        tau=0.001,                   # Slower soft update rate\n",
    "        double_dqn=True,             # Use Double DQN algorithm\n",
    "        prioritized_replay=True,     # Use prioritized experience replay\n",
    "        n_step=3,                    # Use n-step returns\n",
    "        reward_clip=10               # Clip rewards to prevent exploding gradients\n",
    "    )\n",
    "    \n",
    "    # Train agent\n",
    "    trained_agent, rewards, losses = train_improved_dueling_dqn(\n",
    "        env, agent, episodes=episodes, eval_freq=20, log_freq=10\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_results(rewards, losses, \n",
    "                          predictions=trained_agent.predictions, \n",
    "                          true_states=trained_agent.true_states)\n",
    "    \n",
    "    # Evaluate final performance\n",
    "    eval_rewards = []\n",
    "    for _ in range(20):  # Run 20 evaluation episodes\n",
    "        reward = evaluate_episode(env, trained_agent)\n",
    "        eval_rewards.append(reward)\n",
    "    \n",
    "    avg_reward = np.mean(eval_rewards)\n",
    "    print(f\"Final evaluation - Average reward over 20 episodes: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    trained_agent.save_model(\"improved_dueling_dqn_model.pt\")\n",
    "    \n",
    "    return trained_agent\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Starting Improved Dueling DQN experiment on SBEOS environment\")\n",
    "#     trained_agent = run_improved_experiment(episodes=300)\n",
    "#     print(\"Experiment completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dueling DQN experiment with separate training and testing environments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training Improved Dueling DQN:   2%|▏         | 10/500 [01:10<1:01:08,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: -559.20, Avg Reward (last 10): -404.90, Accuracy: 0.4800, Recent Loss: 1.734759, Epsilon: 0.0100\n",
      "New best model saved! Avg reward: -404.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   2%|▏         | 11/500 [01:19<1:02:33,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 225.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   2%|▏         | 12/500 [01:26<1:01:27,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 295.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   4%|▍         | 19/500 [02:26<1:10:19,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, Reward: 782.80, Avg Reward (last 10): 23.04, Accuracy: 0.6600, Recent Loss: 1.604140, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   4%|▍         | 20/500 [02:39<1:20:55, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 20: Reward = 1303.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   4%|▍         | 22/500 [02:56<1:12:39,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 515.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   5%|▍         | 23/500 [03:04<1:09:53,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 945.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   5%|▍         | 24/500 [03:11<1:07:18,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1360.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   5%|▌         | 26/500 [03:29<1:08:36,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1451.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   6%|▌         | 28/500 [03:46<1:07:53,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1495.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   6%|▌         | 29/500 [03:56<1:12:04,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1766.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   6%|▌         | 30/500 [04:06<1:13:05,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30, Reward: 126.80, Avg Reward (last 10): 1700.50, Accuracy: 0.6300, Recent Loss: 1.603673, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   6%|▋         | 32/500 [04:24<1:10:49,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1859.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   7%|▋         | 35/500 [04:49<1:07:40,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1903.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   7%|▋         | 36/500 [04:58<1:07:17,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 1939.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   8%|▊         | 39/500 [05:25<1:07:29,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, Reward: 609.40, Avg Reward (last 10): 1795.02, Accuracy: 0.5780, Recent Loss: 1.388807, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:   8%|▊         | 40/500 [05:36<1:14:12,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 40: Reward = 3525.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  10%|█         | 50/500 [07:12<1:12:47,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Reward: 238.60, Avg Reward (last 10): 1677.66, Accuracy: 0.6220, Recent Loss: 1.439902, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  12%|█▏        | 59/500 [08:50<1:23:14, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, Reward: 2081.20, Avg Reward (last 10): 1825.74, Accuracy: 0.6740, Recent Loss: 1.399248, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  12%|█▏        | 60/500 [09:04<1:29:34, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 60: Reward = -699.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  12%|█▏        | 61/500 [09:15<1:26:27, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 2042.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  12%|█▏        | 62/500 [09:26<1:24:09, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 2219.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  14%|█▍        | 70/500 [10:57<1:23:01, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 70, Reward: -453.80, Avg Reward (last 10): 1059.88, Accuracy: 0.6200, Recent Loss: 1.519426, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  16%|█▌        | 79/500 [12:49<1:31:30, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, Reward: 2861.40, Avg Reward (last 10): 2153.62, Accuracy: 0.7160, Recent Loss: 1.643368, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  16%|█▌        | 80/500 [13:07<1:42:57, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 80: Reward = 1797.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  17%|█▋        | 85/500 [14:14<1:35:42, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved! Avg reward: 2444.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  18%|█▊        | 90/500 [15:19<1:29:20, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 90, Reward: 2486.60, Avg Reward (last 10): 2150.70, Accuracy: 0.6480, Recent Loss: 1.428580, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  20%|█▉        | 99/500 [17:30<1:40:09, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Reward: 1704.60, Avg Reward (last 10): 1052.66, Accuracy: 0.5880, Recent Loss: 1.637999, Epsilon: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  20%|██        | 100/500 [17:46<1:41:46, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 100: Reward = 1134.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Improved Dueling DQN:  20%|██        | 102/500 [18:13<1:36:41, 14.58s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_curve, roc_curve, auc\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "from scipy.stats import entropy\n",
    "import copy\n",
    "from environments import SBEOS_Environment\n",
    "\n",
    "def create_test_environment(seed=None):\n",
    "    \"\"\"Create a separate test environment with a different random seed\"\"\"\n",
    "    # Use a different seed for the test environment to ensure different data generation\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the numpy random seed\n",
    "    \n",
    "    test_env = SBEOS_Environment(max_timesteps=500, window_size=25,time_dependence=time_dependence)\n",
    "    return test_env\n",
    "\n",
    "\n",
    "def evaluate_on_test_environment(agent, test_env, num_episodes=100):\n",
    "    \"\"\"Evaluate agent performance on a separate test environment\"\"\"\n",
    "    test_rewards = []\n",
    "    test_predictions = []\n",
    "    test_true_states = []\n",
    "    \n",
    "    print(\"\\nEvaluating on test environment...\")\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state = test_env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        episode_predictions = []\n",
    "        episode_true_states = []\n",
    "        \n",
    "        # Run until episode ends\n",
    "        while not done:\n",
    "            # Select action without exploration\n",
    "            action = agent.select_action(state, evaluate=True)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = test_env.step(action)\n",
    "            \n",
    "            # Record prediction and true state\n",
    "            episode_predictions.append(action)\n",
    "            episode_true_states.append(info[\"state\"])\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Store episode results\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_predictions.extend(episode_predictions)\n",
    "        test_true_states.extend(episode_true_states)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Test episode {i+1}/{num_episodes} completed. Reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_test_reward = np.mean(test_rewards)\n",
    "    test_accuracy = accuracy_score(test_true_states, test_predictions)\n",
    "    \n",
    "    print(f\"\\nTest Results Summary:\")\n",
    "    print(f\"Average reward: {avg_test_reward:.2f}\")\n",
    "    print(f\"Prediction accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Predictions\",test_predictions)\n",
    "    print(\"Actual\",test_true_states)\n",
    "    return {\n",
    "        'rewards': test_rewards,\n",
    "        'predictions': test_predictions,\n",
    "        'true_states': test_true_states,\n",
    "        'avg_reward': avg_test_reward,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_test_results(test_results):\n",
    "    \"\"\"Plot comprehensive test results including metrics and visualizations\"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot test rewards\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(test_results['rewards'])\n",
    "    plt.plot(pd.Series(test_results['rewards']).rolling(5).mean(), 'r-', label='Moving Average (5)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Test Rewards (Avg: {test_results[\"avg_reward\"]:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.subplot(2, 3, 2)\n",
    "    cm = confusion_matrix(test_results['true_states'][-200:], test_results['predictions'][-200:])\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix (Acc: {test_results[\"accuracy\"]:.4f})')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['State 0', 'State 1']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True State')\n",
    "    plt.xlabel('Predicted State')\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.subplot(2, 3, 3)\n",
    "    # Convert binary predictions to probabilities for ROC curve\n",
    "    y_true = np.array(test_results['true_states'])\n",
    "    y_pred = np.array(test_results['predictions'])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot prediction vs true state over time\n",
    "    plt.subplot(2, 3, 4)\n",
    "    # Sample a subset of the data for readability\n",
    "    sample_size = min(500, len(test_results['predictions']))\n",
    "    sample_indices = np.linspace(0, len(test_results['predictions'])-1, sample_size, dtype=int)\n",
    "    \n",
    "    plt.plot(np.array(test_results['true_states'])[sample_indices], 'b-', label='True State')\n",
    "    plt.plot(np.array(test_results['predictions'])[sample_indices], 'r.', label='Predicted State')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('State')\n",
    "    plt.title('Prediction vs True State')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot class-wise precision and recall\n",
    "    plt.subplot(2, 3, 5)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(recall, precision, label=f'Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot histogram of prediction errors\n",
    "    plt.subplot(2, 3, 6)\n",
    "    errors = np.array(test_results['predictions']) != np.array(test_results['true_states'])\n",
    "    error_indices = np.where(errors)[0]\n",
    "    \n",
    "    # Create a histogram of error locations\n",
    "    if len(error_indices) > 0:\n",
    "        plt.hist(error_indices, bins=30)\n",
    "        plt.xlabel('Step Index')\n",
    "        plt.ylabel('Error Count')\n",
    "        plt.title(f'Distribution of Prediction Errors (Total: {len(error_indices)})')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No prediction errors!', ha='center', va='center')\n",
    "        plt.title('Distribution of Prediction Errors')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "def compare_train_test_performance(train_results, test_results):\n",
    "    \"\"\"Compare training and test performance to assess overfitting\"\"\"\n",
    "    train_accuracy = np.mean(np.array(train_results['predictions']) == np.array(train_results['true_states']))\n",
    "    test_accuracy = test_results['accuracy']\n",
    "    \n",
    "    print(\"\\nTraining vs Test Performance:\")\n",
    "    print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Accuracy difference: {abs(train_accuracy - test_accuracy):.4f}\")\n",
    "    \n",
    "    if test_accuracy < 0.9 * train_accuracy:\n",
    "        print(\"WARNING: Possible overfitting detected. Test accuracy is significantly lower than training accuracy.\")\n",
    "    else:\n",
    "        print(\"Model generalizes well to the test environment.\")\n",
    "    \n",
    "    # Create visualization of train vs test metrics\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Compare accuracies\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Training', 'Test'], [train_accuracy, test_accuracy])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # Compare average rewards\n",
    "    train_avg_reward = np.mean(train_results['rewards'])\n",
    "    test_avg_reward = test_results['avg_reward']\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['Training', 'Test'], [train_avg_reward, test_avg_reward])\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Reward Comparison')\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "def run_dueling_dqn_experiment_with_testing(seed=42, episodes=300, test_seed=12345, test_episodes=100):\n",
    "    \"\"\"Run a complete experiment with separate training and testing environments for Dueling DQN\"\"\"\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create training environment\n",
    "    train_env = SBEOS_Environment(max_timesteps=500, window_size=25,time_dependence=time_dependence)\n",
    "    \n",
    "    # Get state and action dimensions\n",
    "    state_dim = len(train_env.generate_observation_state())\n",
    "    action_dim = 2  # Binary prediction: 0 or 1\n",
    "    \n",
    "    # Create Dueling DQN agent\n",
    "    agent = ImprovedDuelingDQNAgent(\n",
    "        state_size=state_dim,\n",
    "        action_size=action_dim,\n",
    "        memory_size=100000,\n",
    "        batch_size=128,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=50000,\n",
    "        learning_rate=0.0001,\n",
    "        target_update_freq=1000,\n",
    "        use_soft_update=True,\n",
    "        tau=0.001,\n",
    "        double_dqn=True,\n",
    "        prioritized_replay=True,\n",
    "        n_step=3,\n",
    "        reward_clip=10\n",
    "    )\n",
    "    \n",
    "    # Train agent\n",
    "    trained_agent, rewards, losses = train_improved_dueling_dqn(train_env, agent, episodes=episodes, eval_freq=20)\n",
    "    \n",
    "    # Collect training results\n",
    "    train_results = {\n",
    "        'rewards': rewards,\n",
    "        'losses': losses,\n",
    "        'predictions': trained_agent.predictions,\n",
    "        'true_states': trained_agent.true_states\n",
    "    }\n",
    "    \n",
    "    # Plot training results\n",
    "    plot_training_results(rewards, losses, \n",
    "                          predictions=trained_agent.predictions, \n",
    "                          true_states=trained_agent.true_states)\n",
    "    \n",
    "    # Create separate test environment with different seed\n",
    "    np.random.seed(test_seed)\n",
    "    test_env = create_test_environment()\n",
    "    \n",
    "    # Evaluate on test environment\n",
    "    test_results = evaluate_on_test_environment(trained_agent, test_env, num_episodes=test_episodes)\n",
    "    \n",
    "    # Plot test results\n",
    "    plot_test_results(test_results)\n",
    "    \n",
    "    # Compare training and test performance\n",
    "    compare_train_test_performance(train_results, test_results)\n",
    "    \n",
    "    # Save the trained model\n",
    "    trained_agent.save_model(\"improved_dueling_dqn_model.pt\")\n",
    "    \n",
    "    print(\"Dueling DQN experiment with separate testing completed!\")\n",
    "    \n",
    "    return trained_agent, train_results, test_results\n",
    "\n",
    "\n",
    "def analyze_model_performance(train_results, test_results):\n",
    "    \"\"\"Conduct deeper analysis of model performance to identify strengths and weaknesses\"\"\"\n",
    "    # Calculate accuracy by timestep position\n",
    "    test_episodes = len(test_results['rewards'])\n",
    "    steps_per_episode = len(test_results['true_states']) // test_episodes\n",
    "    \n",
    "    # Group predictions by position in episode\n",
    "    position_accuracies = []\n",
    "    for pos in range(steps_per_episode):\n",
    "        pos_preds = []\n",
    "        pos_true = []\n",
    "        for ep in range(test_episodes):\n",
    "            idx = ep * steps_per_episode + pos\n",
    "            if idx < len(test_results['predictions']):\n",
    "                pos_preds.append(test_results['predictions'][idx])\n",
    "                pos_true.append(test_results['true_states'][idx])\n",
    "        \n",
    "        if pos_preds:\n",
    "            acc = accuracy_score(pos_true, pos_preds)\n",
    "            position_accuracies.append(acc)\n",
    "    \n",
    "    # Plot accuracy by position\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(position_accuracies)\n",
    "    plt.xlabel('Step Position in Episode')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy by Step Position')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Calculate entropy of state distributions\n",
    "    state_entropy = []\n",
    "    window_size = 50\n",
    "    for i in range(0, len(test_results['true_states']), window_size):\n",
    "        window = test_results['true_states'][i:i+window_size]\n",
    "        if window:\n",
    "            # Count occurrences of each state\n",
    "            unique, counts = np.unique(window, return_counts=True)\n",
    "            probs = counts / len(window)\n",
    "            ent = entropy(probs, base=2)\n",
    "            state_entropy.append(ent)\n",
    "    \n",
    "    # Plot state entropy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(state_entropy)\n",
    "    plt.xlabel('Window Index')\n",
    "    plt.ylabel('State Entropy (bits)')\n",
    "    plt.title('Environmental State Entropy Over Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate transition patterns analysis\n",
    "    transitions = []\n",
    "    for i in range(1, len(test_results['true_states'])):\n",
    "        prev = test_results['true_states'][i-1]\n",
    "        curr = test_results['true_states'][i]\n",
    "        transitions.append((prev, curr))\n",
    "    \n",
    "    # Count transition frequencies\n",
    "    transition_counts = {}\n",
    "    for t in transitions:\n",
    "        if t not in transition_counts:\n",
    "            transition_counts[t] = 0\n",
    "        transition_counts[t] += 1\n",
    "    \n",
    "    # Print transition analysis\n",
    "    print(\"\\nState Transition Analysis:\")\n",
    "    total = len(transitions)\n",
    "    for t, count in sorted(transition_counts.items()):\n",
    "        print(f\"Transition {t[0]} → {t[1]}: {count} times ({count/total*100:.2f}%)\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Dueling DQN experiment with separate training and testing environments\")\n",
    "    trained_agent, train_results, test_results = run_dueling_dqn_experiment_with_testing(\n",
    "        episodes=500,\n",
    "        test_episodes=100\n",
    "    )\n",
    "    \n",
    "    # Run additional performance analysis\n",
    "    analyze_model_performance(train_results, test_results)\n",
    "    \n",
    "    print(\"Experiment completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
