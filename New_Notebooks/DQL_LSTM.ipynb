{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class ReinforcementEnvironment:\n",
    "    def __init__(self, num_bands, energy_cost=2, reward_factor=5, weight=5, max_timestep=180):\n",
    "        self.num_bands = num_bands\n",
    "        self.energy_cost = energy_cost\n",
    "        self.reward_factor = reward_factor\n",
    "        self.max_timestep = max_timestep\n",
    "        self.weight = weight\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.transition_matrixes = {band: {} for band in range(self.num_bands)}\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "    \n",
    "    def init_bands(self):\n",
    "        \"\"\"Initialize each band with two initial signal values (0 or 1)\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # First signal chosen with equal probability\n",
    "            t1 = np.random.choice([0, 1])\n",
    "            \n",
    "            # Second signal chosen with random probability distribution\n",
    "            t_m1 = np.random.rand(2,2)\n",
    "            t_m1 /= t_m1.sum(axis=1,keepdims=True)  # Normalize to create valid probability distribution\n",
    "            t2 = np.random.choice([0, 1], p=t_m1[t1])\n",
    "            # t_m2 = {\n",
    "            #     (0, 0): np.random.rand(2),\n",
    "            #     (0, 1): np.random.rand(2),\n",
    "            #     (1, 0): np.random.rand(2),\n",
    "            #     (1, 1): np.random.rand(2)\n",
    "            # }\n",
    "            # for k in t_m2:\n",
    "            #     t_m2[k] /= t_m2[k].sum()\n",
    "            t_m2 = {\n",
    "            (0, 0): np.random.dirichlet([1, 1]),  # Generates a valid probability distribution over {0,1}\n",
    "            (0, 1): np.random.dirichlet([1, 1]),\n",
    "            (1, 0): np.random.dirichlet([1, 1]),\n",
    "            (1, 1): np.random.dirichlet([1, 1])\n",
    "            }\n",
    "            self.transition_matrixes[band] = t_m2\n",
    "            self.signal_band[band] = [t1, t2]  \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one time step within the environment\n",
    "        \n",
    "        Args:\n",
    "            action: tuple (band, prediction) where band is the selected frequency band\n",
    "                   and prediction is the predicted signal value (0 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (observation, reward, done, info)\n",
    "        \"\"\"\n",
    "        self.current_timestep += 1\n",
    "        \n",
    "        band = action[0]\n",
    "        prediction = action[1]\n",
    "        \n",
    "        reward = self._calculate_reward(self.current_state[band], prediction)\n",
    "        \n",
    "        self.generate_state()\n",
    "        \n",
    "        observation = self.construct_observation_space()\n",
    "        \n",
    "        done = self.current_timestep >= self.max_timestep\n",
    "        \n",
    "        info = {\n",
    "            \"timestep\": self.current_timestep,\n",
    "            \"correct_prediction\": self.current_state[band] == prediction,\n",
    "            \"state\": self.current_state\n",
    "        }\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self, actual_signal, prediction):\n",
    "        \"\"\"Calculate reward based on prediction accuracy and signal value\"\"\"\n",
    "        if actual_signal == prediction:\n",
    "            # Correct prediction\n",
    "            reward = self.reward_factor * self.weight - self.energy_cost\n",
    "        elif actual_signal == 0:\n",
    "            # Incorrect prediction when signal is 0\n",
    "            reward = self.reward_factor - self.energy_cost\n",
    "        else:  # actual_signal == 1\n",
    "            # Incorrect prediction when signal is 1\n",
    "            reward = self.reward_factor - self.energy_cost * self.weight\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def generate_state(self):\n",
    "        \"\"\"Generate next state for all bands based on transition probabilities\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # Get last two signals for this band\n",
    "            p_2 = tuple(self.signal_band[band][-2:])\n",
    "            \n",
    "            t_m2 = self.transition_matrixes[band]\n",
    "            \n",
    "            next_signal = np.random.choice([0, 1], p=t_m2[p_2])\n",
    "            \n",
    "            self.signal_band[band].append(next_signal)\n",
    "            self.signal_band[band].pop(0)\n",
    "        \n",
    "        # Update current state\n",
    "        self.current_state = self.get_current_state()\n",
    "        \n",
    "        return self.current_state\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Return the current state as a list of the most recent signal for each band\"\"\"\n",
    "        return [self.signal_band[band][-1] for band in range(self.num_bands)]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state and return initial observation\"\"\"\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "        return self.construct_observation_space()\n",
    "    \n",
    "    def construct_observation_space(self, window_size=10):\n",
    "        \"\"\"\n",
    "        Construct observation space with entropy calculations for each band\n",
    "        \n",
    "        Args:\n",
    "            window_size: Number of recent signals to consider for entropy calculation\n",
    "            \n",
    "        Returns:\n",
    "            list: Entropy values for each band\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "        for band in range(self.num_bands):\n",
    "            signal_values = np.array(self.signal_band[band][-window_size:])\n",
    "            \n",
    "            if len(signal_values) <= window_size:\n",
    "                entropy_value = 0\n",
    "            else:\n",
    "                value_counts = np.bincount(signal_values, minlength=2)\n",
    "                \n",
    "                probability_distribution = value_counts / len(signal_values)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if np.all(probability_distribution == 0):\n",
    "                    entropy_value = 0\n",
    "                else:\n",
    "                    # Calculate entropy using scipy function\n",
    "                    entropy_value = entropy(probability_distribution, base=2)\n",
    "            \n",
    "            observation.append(entropy_value)\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def soft_reset(self):\n",
    "        self.signal_band = {band: self.signal_band[band][-2:] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.generate_state()\n",
    "        return self.construct_observation_space()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class AdvancedDQNLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, device=None):\n",
    "        \"\"\"\n",
    "        Advanced LSTM-based Deep Q-Network with input dimensionality handling\n",
    "        \n",
    "        Args:\n",
    "        - input_dim: Dimension of input state\n",
    "        - hidden_dim: Number of LSTM hidden units\n",
    "        - output_dim: Number of possible actions\n",
    "        - num_layers: Number of LSTM layers\n",
    "        - device: Torch device (cuda/cpu)\n",
    "        \"\"\"\n",
    "        super(AdvancedDQNLSTM, self).__init__()\n",
    "        \n",
    "        # Explicitly set device\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128).to(self.device)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 256).to(self.device)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(256, 128).to(self.device)\n",
    "        self.fc_out = nn.Linear(128, output_dim).to(self.device)\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.temperature = 1.0\n",
    "        self.min_temperature = 0.1\n",
    "        self.temperature_decay = 0.9995\n",
    "        \n",
    "        # Store input dimensions\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def _prepare_input(self, state):\n",
    "        \"\"\"\n",
    "        Prepare input tensor for LSTM with proper dimensionality\n",
    "        \n",
    "        Args:\n",
    "        - state: Input state (can be 1D, 2D, or 3D)\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor with dimensions (batch_size, sequence_length, input_dim)\n",
    "        \"\"\"\n",
    "        # Ensure input is a torch tensor\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        \n",
    "        # Move to correct device\n",
    "        state = state.to(self.device)\n",
    "        \n",
    "        # Handle different input dimensionalities\n",
    "        if state.dim() == 1:\n",
    "            # Single 1D state: convert to 3D (batch_size=1, sequence_length=1, input_dim)\n",
    "            state = state.unsqueeze(0).unsqueeze(0)\n",
    "        elif state.dim() == 2:\n",
    "            # 2D input: assume (batch_size, input_dim) - convert to (batch_size, sequence_length=1, input_dim)\n",
    "            state = state.unsqueeze(1)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def forward(self, x, prev_hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the network with input dimensionality handling\n",
    "        \"\"\"\n",
    "        # Prepare input tensor\n",
    "        x = self._prepare_input(x)\n",
    "        \n",
    "        # If no previous hidden state, initialize on the correct device\n",
    "        if prev_hidden is None:\n",
    "            h0 = torch.zeros(\n",
    "                self.lstm.num_layers, \n",
    "                x.size(0), \n",
    "                self.lstm.hidden_size, \n",
    "                device=self.device\n",
    "            )\n",
    "            c0 = torch.zeros(\n",
    "                self.lstm.num_layers, \n",
    "                x.size(0), \n",
    "                self.lstm.hidden_size, \n",
    "                device=self.device\n",
    "            )\n",
    "            prev_hidden = (h0, c0)\n",
    "        else:\n",
    "            # Move previous hidden state to correct device if needed\n",
    "            prev_hidden = (\n",
    "                prev_hidden[0].to(self.device), \n",
    "                prev_hidden[1].to(self.device)\n",
    "            )\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (hidden, cell) = self.lstm(x, prev_hidden)\n",
    "        \n",
    "        # Take the last time step\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(last_time_step))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Output layer\n",
    "        q_values = self.fc_out(x)\n",
    "        \n",
    "        return q_values, (hidden, cell)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Softmax action selection with temperature decay\n",
    "        \n",
    "        Returns an action in the format expected by the environment\n",
    "        \"\"\"\n",
    "        # Decay temperature\n",
    "        self.temperature = max(\n",
    "            self.min_temperature, \n",
    "            self.temperature * (self.temperature_decay ** (1 / 500))\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Prepare input and get Q-values\n",
    "            state_tensor = self._prepare_input(state)\n",
    "            q_values, _ = self(state_tensor)\n",
    "            \n",
    "            # Ensure q_values is 1D numpy array\n",
    "            q_values = q_values.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Softmax action selection\n",
    "            scaled_q_values = q_values / self.temperature\n",
    "            scaled_q_values -= np.max(scaled_q_values)\n",
    "            \n",
    "            exp_q = np.exp(scaled_q_values)\n",
    "            action_probs = exp_q / np.sum(exp_q)\n",
    "            \n",
    "            # Select action index\n",
    "            return np.random.choice(len(q_values), p=action_probs)\n",
    "\n",
    "\n",
    "class ImprovedDQNLSTMAgent:\n",
    "    def __init__(self, env, input_dim, output_dim, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Improved DQN Agent with LSTM and advanced training techniques\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Centralized device selection\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Twin networks with explicit device management\n",
    "        self.q_network = AdvancedDQNLSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            output_dim, \n",
    "            device=self.device\n",
    "        )\n",
    "        self.target_network = AdvancedDQNLSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            output_dim, \n",
    "            device=self.device\n",
    "        )\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.0005\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Experience Replay\n",
    "        self.replay_memory = deque(maxlen=20000)\n",
    "        self.batch_size = 128\n",
    "        \n",
    "        # Optimizer with L2 regularization\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.q_network.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = F.smooth_l1_loss\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store transition in replay memory\n",
    "        \"\"\"\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.replay_memory.append(experience)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Enhanced experience replay with double DQN\n",
    "        \"\"\"\n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)\n",
    "        \n",
    "        # Prepare batch tensors\n",
    "        states = torch.tensor(\n",
    "            np.array([b[0] for b in batch]), \n",
    "            dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        actions = torch.tensor(\n",
    "            [b[1] for b in batch], \n",
    "            dtype=torch.long\n",
    "        ).to(self.device)\n",
    "        \n",
    "        rewards = torch.tensor(\n",
    "            np.array([b[2] for b in batch]), \n",
    "            dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        next_states = torch.tensor(\n",
    "            np.array([b[3] for b in batch]), \n",
    "            dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        dones = torch.tensor(\n",
    "            np.array([b[4] for b in batch]), \n",
    "            dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Compute current Q values\n",
    "        current_q_values, _ = self.q_network(states)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q values with double DQN\n",
    "        with torch.no_grad():\n",
    "            next_q_values_main, _ = self.q_network(next_states)\n",
    "            next_q_values_target, _ = self.target_network(next_states)\n",
    "            \n",
    "            # Double DQN: select actions from main network, evaluate from target\n",
    "            max_next_actions = next_q_values_main.argmax(1)\n",
    "            max_next_q_values = next_q_values_target.gather(1, max_next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute target Q values\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, episodes=1000):\n",
    "        \"\"\"\n",
    "        Training loop with comprehensive metrics\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            # Reset environment\n",
    "            state = self.env.soft_reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.q_network.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                self.store_transition(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                self.experience_replay()\n",
    "            \n",
    "            # Periodic target network update\n",
    "            if episode % 100 == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "            \n",
    "            # Logging\n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Episode {episode}, \"\n",
    "                      f\"Total Reward: {total_reward:.2f}, \"\n",
    "                      f\"Temperature: {self.q_network.temperature:.4f}\")\n",
    "        \n",
    "        return total_rewards\n",
    "\n",
    "# Utility function for plotting rewards\n",
    "def plot_rewards(rewards, window_size=10):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    plt.figure(figsize=(12, 6), facecolor='white')\n",
    "    \n",
    "    plt.plot(rewards, alpha=0.5, color='lightblue', label='Episode Reward')\n",
    "    \n",
    "    moving_average = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    plt.plot(\n",
    "        np.arange(window_size-1, len(rewards)), \n",
    "        moving_average, \n",
    "        color='blue', \n",
    "        linewidth=2, \n",
    "        label=f'{window_size}-Episode Moving Avg'\n",
    "    )\n",
    "    \n",
    "    plt.title('Training Reward over Episodes', fontweight='bold')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM-based DQN\n",
    "    \"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Environment configuration\n",
    "    num_bands = 10\n",
    "    \n",
    "    # Create environment\n",
    "    env = ReinforcementEnvironment(num_bands)\n",
    "    \n",
    "    # Network configuration\n",
    "    input_dim = num_bands  # Input dimension (state size)\n",
    "    output_dim = num_bands * 2  # Output dimension (number of possible actions)\n",
    "    hidden_dim = 64  # LSTM hidden dimension\n",
    "    \n",
    "    # Create agent\n",
    "    agent = ImprovedDQNLSTMAgent(\n",
    "        env=env, \n",
    "        input_dim=input_dim, \n",
    "        output_dim=output_dim, \n",
    "        hidden_dim=hidden_dim\n",
    "    )\n",
    "    \n",
    "    # Train the agent\n",
    "    rewards = agent.train(episodes=500)\n",
    "    \n",
    "    # Plot training rewards\n",
    "    plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m agent \u001b[38;5;241m=\u001b[39m ImprovedDQNLSTMAgent(\n\u001b[1;32m     23\u001b[0m     env\u001b[38;5;241m=\u001b[39menv, \n\u001b[1;32m     24\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim, \n\u001b[1;32m     25\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39moutput_dim, \n\u001b[1;32m     26\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Plot training rewards\u001b[39;00m\n\u001b[1;32m     33\u001b[0m plot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[2], line 287\u001b[0m, in \u001b[0;36mImprovedDQNLSTMAgent.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    286\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m--> 287\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m    291\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mReinforcementEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mExecute one time step within the environment\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    tuple: (observation, reward, done, info)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 57\u001b[0m band \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m prediction \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     60\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_state[band], prediction)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
