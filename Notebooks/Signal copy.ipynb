{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcemnetEnvironment():\n",
    "    def __init__(self,num_bands,energy_cost=0.2,reward_factor=5,weight=2,max_timestep=180):\n",
    "        self.num_bands = num_bands\n",
    "        self.energy_cost = energy_cost\n",
    "        self.reward_factor = reward_factor\n",
    "        self.max_timestep = max_timestep\n",
    "        self.weight = weight\n",
    "        self.signal_band = {band:[] for band in range(self.num_bands)}\n",
    "        self.init_bands()\n",
    "    \n",
    "    def init_bands(self):\n",
    "        for band in range(self.num_bands):\n",
    "            t1 = np.random.choice([0,1])\n",
    "            t_m1 = np.random.rand(2)\n",
    "            t_m1 /= t_m1.sum()\n",
    "            t2 = np.random.choice([0,1],p=t_m1)\n",
    "            self.signal_band[band].append(t1)\n",
    "            self.signal_band[band].append(t2)\n",
    "    \n",
    "    \n",
    "    def step(self,state,action):\n",
    "        reward = 0\n",
    "        band = action[0]\n",
    "        prediction = action[1]\n",
    "        \n",
    "        if state[band] == prediction:\n",
    "            reward = self.reward_factor*self.weight - self.energy_cost\n",
    "        elif state[band] != prediction and state[band] == 0:\n",
    "            reward = self.reward_factor - self.energy_cost\n",
    "        elif state[band] != prediction and state[band] == 1:\n",
    "            reward = self.reward_factor - self.energy_cost*2\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def generate_state(self):\n",
    "        for band in range(self.num_bands):\n",
    "            p_2 = tuple(self.signal_band[band][-2:])\n",
    "            t_m2 = {\n",
    "                (0,0):np.random.rand(2),\n",
    "                (0,1):np.random.rand(2),\n",
    "                (1,0):np.random.rand(2),\n",
    "                (1,1):np.random.rand(2)\n",
    "            }\n",
    "            \n",
    "            for k in t_m2:\n",
    "                t_m2[k] /= t_m2[k].sum()\n",
    "            \n",
    "            nx = np.random.choice([0,1],p=t_m2[p_2])\n",
    "            self.signal_band[band].append(nx)\n",
    "            self.signal_band[band].pop(0)\n",
    "        \n",
    "        state = [self.signal_band[val][-1] for val in self.signal_band] \n",
    "        return state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.signal_band = self.signal_band = {band:[] for band in range(self.num_bands)}\n",
    "        self.init_bands()\n",
    "    \n",
    "    def construct_observation_space(self,window_size=2):\n",
    "        observation = []\n",
    "        for band in range(self.num_bands):\n",
    "            signal_values = np.array(self.signal_band[band][-window_size:])\n",
    "            if len(signal_values) == 0:\n",
    "                entropy_value = 0\n",
    "            else:  \n",
    "                value_counts = np.bincount(signal_values, minlength=2)  \n",
    "                probability_distribution = value_counts / value_counts.sum()  \n",
    "\n",
    "                if probability_distribution.sum() == 0:\n",
    "                    entropy_value = 0  \n",
    "                else:\n",
    "                    entropy_value = entropy(probability_distribution, base=2)  \n",
    "            \n",
    "            observation.append(entropy_value)\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "        \n",
    "    \n",
    "                \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class ReinforcementEnvironment:\n",
    "    def __init__(self, num_bands, energy_cost=0.2, reward_factor=5, weight=2, max_timestep=180):\n",
    "        self.num_bands = num_bands\n",
    "        self.energy_cost = energy_cost\n",
    "        self.reward_factor = reward_factor\n",
    "        self.max_timestep = max_timestep\n",
    "        self.weight = weight\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.transition_matrixes = {band: {} for band in range(self.num_bands)}\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "    \n",
    "    def init_bands(self):\n",
    "        \"\"\"Initialize each band with two initial signal values (0 or 1)\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # First signal chosen with equal probability\n",
    "            t1 = np.random.choice([0, 1])\n",
    "            \n",
    "            # Second signal chosen with random probability distribution\n",
    "            t_m1 = np.random.rand(2,2)\n",
    "            t_m1 /= t_m1.sum(axis=1,keepdims=True)  # Normalize to create valid probability distribution\n",
    "            t2 = np.random.choice([0, 1], p=t_m1[t1])\n",
    "            t_m2 = {\n",
    "                (0, 0): np.random.rand(2),\n",
    "                (0, 1): np.random.rand(2),\n",
    "                (1, 0): np.random.rand(2),\n",
    "                (1, 1): np.random.rand(2)\n",
    "            }\n",
    "            for k in t_m2:\n",
    "                t_m2[k] /= t_m2[k].sum()\n",
    "            self.transition_matrixes[band] = t_m2\n",
    "            self.signal_band[band] = [t1, t2]  # Store initial values\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one time step within the environment\n",
    "        \n",
    "        Args:\n",
    "            action: tuple (band, prediction) where band is the selected frequency band\n",
    "                   and prediction is the predicted signal value (0 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (observation, reward, done, info)\n",
    "        \"\"\"\n",
    "        # Increment timestep\n",
    "        self.current_timestep += 1\n",
    "        \n",
    "        # Parse action\n",
    "        band = action[0]\n",
    "        prediction = action[1]\n",
    "        \n",
    "        # Calculate reward based on current state and action\n",
    "        reward = self._calculate_reward(self.current_state[band], prediction)\n",
    "        \n",
    "        # Generate next state\n",
    "        self.generate_state()\n",
    "        \n",
    "        # Get observation for the agent\n",
    "        observation = self.construct_observation_space()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_timestep >= self.max_timestep\n",
    "        \n",
    "        # Additional info\n",
    "        info = {\n",
    "            \"timestep\": self.current_timestep,\n",
    "            \"correct_prediction\": self.current_state[band] == prediction,\n",
    "            \"state\": self.current_state\n",
    "        }\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self, actual_signal, prediction):\n",
    "        \"\"\"Calculate reward based on prediction accuracy and signal value\"\"\"\n",
    "        if actual_signal == prediction:\n",
    "            # Correct prediction\n",
    "            reward = self.reward_factor * self.weight - self.energy_cost\n",
    "        elif actual_signal == 0:\n",
    "            # Incorrect prediction when signal is 0\n",
    "            reward = self.reward_factor - self.energy_cost\n",
    "        else:  # actual_signal == 1\n",
    "            # Incorrect prediction when signal is 1\n",
    "            reward = self.reward_factor - self.energy_cost * self.weight\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def generate_state(self):\n",
    "        \"\"\"Generate next state for all bands based on transition probabilities\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # Get last two signals for this band\n",
    "            p_2 = tuple(self.signal_band[band][-2:])\n",
    "            \n",
    "            # Create random transition probabilities for all possible previous states\n",
    "            t_m2 = self.transition_matrixes[band]\n",
    "            \n",
    "            # Generate next signal based on transition probability\n",
    "            next_signal = np.random.choice([0, 1], p=t_m2[p_2])\n",
    "            \n",
    "            # Update signal history for this band\n",
    "            self.signal_band[band].append(next_signal)\n",
    "            self.signal_band[band].pop(0)\n",
    "        \n",
    "        # Update current state\n",
    "        self.current_state = self.get_current_state()\n",
    "        \n",
    "        return self.current_state\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Return the current state as a list of the most recent signal for each band\"\"\"\n",
    "        return [self.signal_band[band][-1] for band in range(self.num_bands)]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state and return initial observation\"\"\"\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "        return self.construct_observation_space()\n",
    "    \n",
    "    def construct_observation_space(self, window_size=2):\n",
    "        \"\"\"\n",
    "        Construct observation space with entropy calculations for each band\n",
    "        \n",
    "        Args:\n",
    "            window_size: Number of recent signals to consider for entropy calculation\n",
    "            \n",
    "        Returns:\n",
    "            list: Entropy values for each band\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "        for band in range(self.num_bands):\n",
    "            # Get recent signals for this band\n",
    "            signal_values = np.array(self.signal_band[band][-window_size:])\n",
    "            \n",
    "            if len(signal_values) == 0:\n",
    "                entropy_value = 0\n",
    "            else:\n",
    "                # Count occurrences of each value (0 or 1)\n",
    "                value_counts = np.bincount(signal_values, minlength=2)\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probability_distribution = value_counts / len(signal_values)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if np.all(probability_distribution == 0):\n",
    "                    entropy_value = 0\n",
    "                else:\n",
    "                    # Calculate entropy using scipy function\n",
    "                    entropy_value = entropy(probability_distribution, base=2)\n",
    "            \n",
    "            observation.append(entropy_value)\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def soft_reset(self):\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "        return self.construct_observation_space()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class ReinforcementEnvironment:\n",
    "    def __init__(self, num_bands, energy_cost=0.2, reward_factor=5, weight=2, max_timestep=180):\n",
    "        self.num_bands = num_bands\n",
    "        self.energy_cost = energy_cost\n",
    "        self.reward_factor = reward_factor\n",
    "        self.max_timestep = max_timestep\n",
    "        self.weight = weight\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.transition_matrixes = {band: {} for band in range(self.num_bands)}\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "    \n",
    "    def init_bands(self):\n",
    "        \"\"\"Initialize each band with two initial signal values (0 or 1)\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # First signal chosen with equal probability\n",
    "            t1 = np.random.choice([0, 1])\n",
    "            \n",
    "            # Second signal chosen with random probability distribution\n",
    "            t_m1 = np.random.rand(2,2)\n",
    "            t_m1 /= t_m1.sum(axis=1,keepdims=True)  # Normalize to create valid probability distribution\n",
    "            t2 = np.random.choice([0, 1], p=t_m1[t1])\n",
    "            t_m2 = {\n",
    "                (0, 0): np.random.rand(2),\n",
    "                (0, 1): np.random.rand(2),\n",
    "                (1, 0): np.random.rand(2),\n",
    "                (1, 1): np.random.rand(2)\n",
    "            }\n",
    "            for k in t_m2:\n",
    "                t_m2[k] /= t_m2[k].sum()\n",
    "            self.transition_matrixes[band] = t_m2\n",
    "            self.signal_band[band] = [t1, t2]  # Store initial values\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one time step within the environment\n",
    "        \n",
    "        Args:\n",
    "            action: tuple (band, prediction) where band is the selected frequency band\n",
    "                and prediction is the predicted signal value (0 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (observation, reward, done, info)\n",
    "        \"\"\"\n",
    "        # Increment timestep\n",
    "        self.current_timestep += 1\n",
    "        \n",
    "        # Parse action\n",
    "        band = action[0]\n",
    "        prediction = action[1]\n",
    "        \n",
    "        # Calculate reward based on current state and action\n",
    "        reward = self._calculate_reward(self.current_state[band], prediction)\n",
    "        \n",
    "        # Generate next state\n",
    "        self.generate_state()\n",
    "        \n",
    "        # Get observation for the agent\n",
    "        observation = self.construct_observation_space()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_timestep >= self.max_timestep\n",
    "        \n",
    "        # Additional info\n",
    "        info = {\n",
    "            \"timestep\": self.current_timestep,\n",
    "            \"correct_prediction\": self.current_state[band] == prediction,\n",
    "            \"state\": self.current_state\n",
    "        }\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self, actual_signal, prediction):\n",
    "        \"\"\"Calculate reward based on prediction accuracy and signal value\"\"\"\n",
    "        if actual_signal == prediction:\n",
    "            # Correct prediction\n",
    "            reward = self.reward_factor * self.weight - self.energy_cost\n",
    "        elif actual_signal == 0:\n",
    "            # Incorrect prediction when signal is 0\n",
    "            reward = self.reward_factor - self.energy_cost\n",
    "        else:  # actual_signal == 1\n",
    "            # Incorrect prediction when signal is 1\n",
    "            reward = self.reward_factor - self.energy_cost * self.weight\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def generate_state(self):\n",
    "        \"\"\"Generate next state for all bands based on transition probabilities\"\"\"\n",
    "        for band in range(self.num_bands):\n",
    "            # Get last two signals for this band\n",
    "            p_2 = tuple(self.signal_band[band][-2:])\n",
    "            \n",
    "            # Create random transition probabilities for all possible previous states\n",
    "            t_m2 = self.transition_matrixes[band]\n",
    "            # Generate next signal based on transition probability\n",
    "            next_signal = np.random.choice([0, 1], p=t_m2[p_2])\n",
    "            \n",
    "            # Update signal history for this band\n",
    "            self.signal_band[band].append(next_signal)\n",
    "            self.signal_band[band].pop(0)\n",
    "        \n",
    "        # Update current state\n",
    "        self.current_state = self.get_current_state()\n",
    "        \n",
    "        return self.current_state\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Return the current state as a list of the most recent signal for each band\"\"\"\n",
    "        return [self.signal_band[band][-1] for band in range(self.num_bands)]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state and return initial observation\"\"\"\n",
    "        self.signal_band = {band: [] for band in range(self.num_bands)}\n",
    "        self.current_timestep = 0\n",
    "        self.init_bands()\n",
    "        self.current_state = self.get_current_state()\n",
    "        return self.construct_observation_space()\n",
    "    \n",
    "    def construct_observation_space(self, window_size=2):\n",
    "        \"\"\"\n",
    "        Construct observation space with entropy calculations and transition probabilities for each band\n",
    "        \n",
    "        Args:\n",
    "            window_size: Number of recent signals to consider for entropy calculation\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains entropy values and transition probabilities for each band\n",
    "        \"\"\"\n",
    "        observation = {}\n",
    "        for band in range(self.num_bands):\n",
    "            # Get recent signals for this band\n",
    "            signal_values = np.array(self.signal_band[band])\n",
    "            \n",
    "            # Calculate entropy\n",
    "            if len(signal_values) < window_size:\n",
    "                entropy_value = 0\n",
    "            else:\n",
    "                recent_signals = signal_values[-window_size:]\n",
    "                # Count occurrences of each value (0 or 1)\n",
    "                value_counts = np.bincount(recent_signals, minlength=2)\n",
    "                \n",
    "                # Calculate probability distribution\n",
    "                probability_distribution = value_counts / len(recent_signals)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if np.all(probability_distribution == 0):\n",
    "                    entropy_value = 0\n",
    "                else:\n",
    "                    # Calculate entropy using scipy function\n",
    "                    entropy_value = entropy(probability_distribution, base=2)\n",
    "            \n",
    "            # Calculate transition probabilities (0->1 and 1->0)\n",
    "            transitions_0to1 = 0\n",
    "            transitions_1to0 = 0\n",
    "            total_0 = 0\n",
    "            total_1 = 0\n",
    "            \n",
    "            # We need at least 2 signals to calculate transitions\n",
    "            if len(signal_values) >= 2:\n",
    "                # Count transitions\n",
    "                for i in range(len(signal_values)-1):\n",
    "                    if signal_values[i] == 0:\n",
    "                        total_0 += 1\n",
    "                        if signal_values[i+1] == 1:\n",
    "                            transitions_0to1 += 1\n",
    "                    elif signal_values[i] == 1:\n",
    "                        total_1 += 1\n",
    "                        if signal_values[i+1] == 0:\n",
    "                            transitions_1to0 += 1\n",
    "            \n",
    "            # Calculate probabilities\n",
    "            prob_0to1 = transitions_0to1 / total_0 if total_0 > 0 else 0\n",
    "            prob_1to0 = transitions_1to0 / total_1 if total_1 > 0 else 0\n",
    "            \n",
    "            # Store all information for this band\n",
    "            observation[band] = {\n",
    "                'entropy': entropy_value,\n",
    "                'prob_0to1': prob_0to1,\n",
    "                'prob_1to0': prob_1to0\n",
    "            }\n",
    "        \n",
    "        return observation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
