{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import math\n",
    "\n",
    "# Define transition tuple\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy Linear layer for exploration\"\"\"\n",
    "    def __init__(self, in_features, out_features, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        \n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
    "        \n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001, n_step=3, gamma=0.99):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.eps = 1e-5\n",
    "        \n",
    "        # N-step learning\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "    \n",
    "    def _get_n_step_info(self):\n",
    "        reward, next_state, done = self.n_step_buffer[-1][-3:]\n",
    "        \n",
    "        for transition in reversed(list(self.n_step_buffer)[:-1]):\n",
    "            r, n_s, d = transition[-3:]\n",
    "            \n",
    "            reward = r + self.gamma * reward * (1 - d)\n",
    "            next_state = n_s if d else next_state\n",
    "            done = d if d else done\n",
    "        \n",
    "        return reward, next_state, done\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Store transition in n-step buffer\n",
    "        self.n_step_buffer.append(Transition(*args))\n",
    "        \n",
    "        # If we don't have enough transitions yet, return\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return\n",
    "        \n",
    "        # Get n-step transition\n",
    "        reward, next_state, done = self._get_n_step_info()\n",
    "        state, action = self.n_step_buffer[0][:2]\n",
    "        \n",
    "        # Store n-step transition in main buffer\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = Transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            indices = range(len(self.buffer))\n",
    "        else:\n",
    "            priorities = self.priorities[:len(self.buffer)]\n",
    "            probabilities = priorities ** self.alpha\n",
    "            probabilities /= probabilities.sum()\n",
    "            \n",
    "            indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        \n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        return samples, indices, torch.FloatTensor(weights)\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority + self.eps\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, atom_size=51, v_min=-10, v_max=10):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        \n",
    "        # Distributional RL parameters\n",
    "        self.atom_size = atom_size\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.support = torch.linspace(v_min, v_max, atom_size)\n",
    "        \n",
    "        # Common layers\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        \n",
    "        # Dueling architecture\n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(64, atom_size)\n",
    "        )\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(64, action_dim * atom_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_stream(x).view(-1, 1, self.atom_size)\n",
    "        advantage = self.advantage_stream(x).view(-1, self.action_dim, self.atom_size)\n",
    "        \n",
    "        # Combine value and advantage using dueling formula\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Get probabilities with softmax\n",
    "        q_dist = F.softmax(q_atoms, dim=2)\n",
    "        \n",
    "        return q_dist\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset noise for exploration\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Get action with highest expected value\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_dist = self.forward(state)\n",
    "            q_values = (q_dist * self.support).sum(dim=2)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "class RainbowAgent:\n",
    "    def __init__(self, state_dim, action_dim, buffer_size=100000, batch_size=64, \n",
    "                 gamma=0.99, lr=0.0001, target_update=1000, num_atoms=51, \n",
    "                 v_min=-10, v_max=10, n_step=3, alpha=0.5, beta=0.4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        # Distributional RL parameters\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.support = torch.linspace(v_min, v_max, num_atoms)\n",
    "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "        \n",
    "        # Networks\n",
    "        self.online_net = RainbowDQN(state_dim, action_dim, num_atoms, v_min, v_max)\n",
    "        self.target_net = RainbowDQN(state_dim, action_dim, num_atoms, v_min, v_max)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            buffer_size, alpha=alpha, beta=beta, n_step=n_step, gamma=gamma\n",
    "        )\n",
    "        \n",
    "        # Update counter\n",
    "        self.update_count = 0\n",
    "    \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"Select action based on current policy\"\"\"\n",
    "        if evaluate:\n",
    "            return self.online_net.act(state)\n",
    "        else:\n",
    "            # Noisy network provides exploration, no epsilon needed\n",
    "            return self.online_net.act(state)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update network parameters\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.FloatTensor(batch.state)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1)\n",
    "        next_state_batch = torch.FloatTensor(batch.next_state)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1)\n",
    "        \n",
    "        # Get current Q distributions\n",
    "        current_q_dist = self.online_net(state_batch)\n",
    "        current_q_dist = current_q_dist.gather(1, action_batch.unsqueeze(-1).expand(-1, -1, self.num_atoms)).squeeze(1)\n",
    "        \n",
    "        # Get next Q distributions (for double DQN)\n",
    "        with torch.no_grad():\n",
    "            # Get argmax actions from online network\n",
    "            next_q_dist = self.online_net(next_state_batch)\n",
    "            next_q = (next_q_dist * self.support).sum(dim=2)\n",
    "            next_actions = next_q.max(1)[1].unsqueeze(1)\n",
    "            \n",
    "            # Get Q distributions from target network using argmax actions\n",
    "            next_q_dist = self.target_net(next_state_batch)\n",
    "            next_q_dist = next_q_dist.gather(1, next_actions.unsqueeze(-1).expand(-1, -1, self.num_atoms)).squeeze(1)\n",
    "            \n",
    "            # Calculate target distribution\n",
    "            Tz = reward_batch + (1 - done_batch) * (self.gamma ** self.memory.n_step) * self.support\n",
    "            Tz = Tz.clamp(self.v_min, self.v_max)\n",
    "            \n",
    "            # Project onto fixed support\n",
    "            b = (Tz - self.v_min) / self.delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "            \n",
    "            # Distribute probability mass\n",
    "            target_q_dist = torch.zeros_like(current_q_dist)\n",
    "            offset = torch.linspace(0, (self.batch_size - 1) * self.num_atoms, self.batch_size).long().unsqueeze(1).expand(self.batch_size, self.num_atoms)\n",
    "            \n",
    "            target_q_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_q_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            target_q_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_q_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "        \n",
    "        # Calculate loss\n",
    "        KL_div = -(target_q_dist * torch.log(current_q_dist + 1e-8)).sum(dim=1)\n",
    "        weighted_loss = (KL_div * weights).mean()\n",
    "        \n",
    "        # Calculate priorities for PER\n",
    "        priorities = KL_div.detach().cpu().numpy()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        # Optional: clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities\n",
    "        self.memory.update_priorities(indices, priorities)\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        \n",
    "        # Reset noise for exploration\n",
    "        self.online_net.reset_noise()\n",
    "        \n",
    "        return weighted_loss.item()\n",
    "\n",
    "def train_rainbow(env, episodes=1000, state_dim=11, gamma=0.99, batch_size=64, update_frequency=4):\n",
    "    \"\"\"\n",
    "    Train using Rainbow DQN\n",
    "    \n",
    "    Args:\n",
    "        env: The SpectrumEnvironment instance\n",
    "        episodes: Number of episodes to train\n",
    "        state_dim: Dimension of state (num_bands + 1 for time step)\n",
    "        gamma: Discount factor\n",
    "        batch_size: Batch size for updates\n",
    "        update_frequency: Steps between network updates\n",
    "    \n",
    "    Returns:\n",
    "        List of episode rewards\n",
    "    \"\"\"\n",
    "    # Number of actions: detect or skip for each band\n",
    "    action_dim = env.num_bands * 2\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = RainbowAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        batch_size=batch_size,\n",
    "        gamma=gamma,\n",
    "        # Customize for spectral sensing\n",
    "        v_min=-100,  # Min expected reward\n",
    "        v_max=100,   # Max expected reward\n",
    "        n_step=3     # Multi-step learning\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    step_count = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        env.soft_reset()\n",
    "        env._generate_spectrum_state()\n",
    "        \n",
    "        # Initialize state (time_step=0)\n",
    "        time_step = 0\n",
    "        state = np.zeros(state_dim)\n",
    "        state[0] = time_step / env.steps  # Normalize time step\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select and take action\n",
    "            action = agent.select_action(state)\n",
    "            next_state_tuple, reward, done = env.step(time_step, action)\n",
    "            \n",
    "            # Update state\n",
    "            next_time_step = next_state_tuple[0]\n",
    "            next_state = np.zeros(state_dim)\n",
    "            next_state[0] = next_time_step / env.steps  # Normalize time step\n",
    "            \n",
    "            # For each band, add its energy level (if available)\n",
    "            for band in range(env.num_bands):\n",
    "                if env.energy_costs[band]:\n",
    "                    next_state[band + 1] = env.energy_costs[band][-1]\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update network\n",
    "            step_count += 1\n",
    "            if step_count % update_frequency == 0:\n",
    "                loss = agent.update()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            time_step = next_time_step\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}, Energy Used: {env.total_energy}\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
